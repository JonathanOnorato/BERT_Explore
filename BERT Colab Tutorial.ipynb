{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c2f734910548c085a0e5761ac42c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print(\"Loading BERT tokenizer\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"UPDATES\\n\\n\\n\\nDOI: 10.1002/adsc.201200179\\n\\nPalladium on Carbon-Catalyzed Cross-Coupling using Triarylbismuths\\n\\n\\n\\nYasunari Monguchi,a,* Tomohiro Hattori,a Yasuhiro Miyamoto,a Takayoshi Yanase,a Yoshinari Sawama,a and Hironao Sajikia,*\\n\\na Laboratory of Organic Chemistry, Gifu Pharmaceutical University, Gifu 501-1196, Japan Fax: (+ 81)-58-230-8109; phone: (+ 81)-58-230-8109; e-mail: monguchi@gifu-pu.ac.jp or sajiki@gifu-pu.ac.jp\\n\\nReceived: March 4, 2012; Revised: May 16, 2012; Published online: August 22, 2012\\n\\nSupporting information for this article is available on the WWW under http://dx.doi.org/10.1002/adsc.201200179.\\n\\n\\n\\nAbstract: Simple and efficient protocols for the 10% palladium on carbon (Pd/C)-catalyzed crosscoupling reactions between triarylbismuths and aryl halides have been developed. A variety of iodoand bromobenzenes possessing an electron-withdrawing group on the aromatic nucleus were smoothly cross-coupled in the presence of 10% Pd/C, sodium phosphate dodecahydrate (Na3PO4·12 H2O) and 1,4-diazabicycloENRT[GUACH 2.2.2]octane (DABCO) in heated N-methyl-2-pyrrolidone (NMP) as the solvent. For the arylations of iodobenzenes, the reactions effectively proceeded under the combined use of caesium fluoride (CsF) and 2,2'-biquinoline. Furthermore, a ligand-free 10% Pd/C-catalyzed cross-coupling reaction between the aryl iodides and triarylbismuths was also established by the addition of tetra-n-buthylammonium fluoride trihydrate (TBAF·3 H2O) in which the palladium metals were hardly leached from the catalyst into the reaction media.\\n\\nKeywords: bismuth; cross-coupling; heterogeneous catalysis; ligand-free conditions; palladium\\n\\nOrganobismuth compounds have attracted a great deal of attention as nucleophilic reagents in organic synthesis due to their harmless properties.[1] Triarylbismuths are particularly useful from environmental and atom-economical points of view since all three aromatic substituents can be efficiently and equally used in the reaction.[2\\xad4] The use of triarylbismuths has recently been studied for the homogeneous palladium-catalyzed cross-coupling reactions with aryl halides[3] as well as acyl chlorides.[4] The cross-coupling reactions between triarylbismuths and aryl halides are expected to be complementary alternatives to the Suzuki\\xadMiyaura, Stille, and Hiyama cross-coupling re-\\n\\n\\n\\nactions, since they form biaryl motifs, which are important structural units for biologically active compounds, such as pharmaceuticals and agrochemicals.\\n\\nAlthough the heterogeneous palladium catalysts have advantages over homogeneous ones, e.g., the air stability and less residual properties,[5] only one application of the heterogeneous Pd(II) catalyst, which is supported on phosphine-linked polystyrene resins,[6] for the cross-coupling reaction of triarylbismuths with aryl halides has been pioneeringly reported.[7] Palladium on carbon (Pd/C), one of the most representative and common catalysts for hydrogenations,[8] has recently found good utilities for a variety of cross-coupling reactions even in the absence of phosphine and amine ligands.[9,10]\\n\\nIn this paper, three different protocols for the Pd/ C-catalyzed cross-coupling reactions of triarylbismuths with aryl halides based on the electron density on the aromatic nuclei are demonstrated, and even electron-rich iodobenzenes, which are widely recognized as less reactive substrates in common cross-coupling reactions as typified by the Suzuki\\xadMiyaura reaction, can undergo smooth cross-coupling in a ligand-free fashion.[11]\\n\\nWhen 4-bromonitrobenzene was stirred with triphenylbismuth in the presence of 10% Pd/C (5 mol%)[12] and potassium fluoride (KF) (2 equiv. vs. each phenyl substituent of triphenylbismuth) in heated DMF (120 8C), the corresponding 4-nitrobiphenyl was produced in 41% yield (Table 1, entry 1). The effect of base on the cross-coupling was initially investigated in order to optimize the reaction conditions. As a result, the addition of sodium phosphate dodecahydrate (Na3PO4·12 H2O) and 1,4-diazabicycloERNACG[HUT 2.2.2]octane (DABCO) produced good results (entries 7 and 10), and the combined use of Na3PO4·12 H2O as a base and 10 mol% of DABCO as an amine ligand was found to dramatically improve the reaction efficiency to form 4-nitrobiphenyl in 65% yield (entry 11). The reaction was affected by the type of solvent; the reac-\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\n2561\\n\\n\\n\\n\\x0cUPDATES\\n\\n\\n\\nYasunari Monguchi et al.\\n\\n\\n\\nTable 1. Optimization of reaction conditions for the cross- Table 2. Pd/C-catalyzed cross-coupling reaction by the com-\\n\\n\\n\\ncoupling of 4-bromonitorobenzene with triphenylbismuth.\\n\\n\\n\\nbined use of Na3PO4·12 H2O and DABCO.\\n\\n\\n\\nEntry Base\\n\\n\\n\\nLigand Solvent Yield [%][a]\\n\\n\\n\\n1\\n\\n\\n\\nKF\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 41\\n\\n\\n\\n2\\n\\n\\n\\nCsF\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 24\\n\\n\\n\\n3\\n\\n\\n\\nTBAF·3 H2O \\xad\\n\\n\\n\\n4\\n\\n\\n\\nCs2CO3\\n\\n\\n\\n\\xad\\n\\n\\n\\n5\\n\\n\\n\\nNaOH\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 26 DMF 26 DMF 22\\n\\n\\n\\n6\\n\\n\\n\\nK3PO4\\n\\n\\n\\n\\xad\\n\\n\\n\\n7\\n\\n\\n\\nNa3PO4·12 H2O \\xad\\n\\n\\n\\n8\\n\\n\\n\\nTMEDA\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 22 DMF 60 DMF 18\\n\\n\\n\\n9\\n\\n\\n\\nDMEDA\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 18\\n\\n\\n\\n10\\n\\n\\n\\nDABCO\\n\\n\\n\\n\\xad\\n\\n\\n\\nDMF 50\\n\\n\\n\\n11\\n\\n\\n\\nNa3PO4·12 H2O DABCO DMF 65\\n\\n\\n\\n12\\n\\n\\n\\nNa3PO4·12 H2O DABCO DMA 78\\n\\n\\n\\n13\\n\\n\\n\\nNa3PO4·12 H2O DABCO DMSO 79\\n\\n\\n\\n14\\n\\n\\n\\nNa3PO4·12 H2O DABCO NMP 80\\n\\n\\n\\n15[b] Na3PO4·12 H2O DABCO toluene 3\\n\\n\\n\\n16[b] Na3PO4·12 H2O DABCO MeCN 4\\n\\n\\n\\n17[b] Na3PO4·12 H2O DABCO MeOH 16\\n\\n\\n\\n18[b] Na3PO4·12 H2O DABCO THF\\n\\n\\n\\n1\\n\\n\\n\\n[a] Determined by 1H NMR analysis using 1,4-dioxane as an internal standard based on a phenyl substituent of triphenylbismuth.\\n\\n[b] Under reflux.\\n\\n\\n\\ntion hardly proceeded in toluene, acetonitrile, methanol and THF (entries 15\\xad18), while aprotic polar solvents such as N,N-dimethylformamide (DMF), N,Ndimethylacetamide (DMA), dimethyl sulfoxide (DMSO), and NMP were effective as a general rule (entries 11\\xad14). Since the reaction in NMP led to a higher yield (entry 14), N-methyl-2-pyrrolidone (NMP) was selected as the optimal solvent.\\n\\nA variety of iodo- and bromobenzenes posseesing an electron-withdrawing functionality on the benzene nucleus were smoothly cross-coupled with triarylbismuths in NMP at 120 8C regardless of the substitution pattern of the halobenzenes (Table 2, entries 1\\xad11). On the other hand, the methyl- or methoxy-substituted, electron-sufficient halobenzenes revealed remarkably low reactivities toward the phenylation using triphenylbismuth under the stated reaction conditions (entries 12\\xad14).\\n\\nAlthough raising the temperature from 120 8C to 140 8C led to only a slight improvement in the reaction efficiency for the cross-coupling using triphenylbismuth and 4-iodoanisole as an electron-sufficient halobenzene (Table 3, entries 1 vs. 2), the reaction was significantly enhanced by the use of caesium fluo-\\n\\n\\n\\nEntry\\n\\n\\n\\nR1\\n\\n\\n\\nR2\\n\\n\\n\\nX\\n\\n\\n\\nYield [%][a]\\n\\n\\n\\n1\\n\\n\\n\\nH\\n\\n\\n\\n4-NO2\\n\\n\\n\\nBr\\n\\n\\n\\n75\\n\\n\\n\\n2\\n\\n\\n\\nH\\n\\n\\n\\n3-NO2\\n\\n\\n\\nBr\\n\\n\\n\\n55\\n\\n\\n\\n3\\n\\n\\n\\nH\\n\\n\\n\\n2-NO2\\n\\n\\n\\nBr\\n\\n\\n\\n75\\n\\n\\n\\n4\\n\\n\\n\\nMeO\\n\\n\\n\\n4-NO2\\n\\n\\n\\nBr\\n\\n\\n\\n83\\n\\n\\n\\n5\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\nBr\\n\\n\\n\\n70\\n\\n\\n\\n6\\n\\n\\n\\nH\\n\\n\\n\\n4-NO2\\n\\n\\n\\nI\\n\\n\\n\\n83\\n\\n\\n\\n7\\n\\n\\n\\nMeO\\n\\n\\n\\n4-NO2\\n\\n\\n\\nI\\n\\n\\n\\n58\\n\\n\\n\\n8\\n\\n\\n\\nH\\n\\n\\n\\n4-CO2Et\\n\\n\\n\\nI\\n\\n\\n\\n75\\n\\n\\n\\n9\\n\\n\\n\\nH\\n\\n\\n\\n4-Cl\\n\\n\\n\\nI\\n\\n\\n\\n59\\n\\n\\n\\n10\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\nI\\n\\n\\n\\n61\\n\\n\\n\\n11\\n\\n\\n\\nMeO\\n\\n\\n\\n4-Ac\\n\\n\\n\\nI\\n\\n\\n\\n52\\n\\n\\n\\n12\\n\\n\\n\\nH\\n\\n\\n\\n4-Me\\n\\n\\n\\nBr\\n\\n\\n\\n3\\n\\n\\n\\n13\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\nBr\\n\\n\\n\\ntrace\\n\\n\\n\\n14\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\nI\\n\\n\\n\\n20\\n\\n\\n\\n[a] Determined by 1H NMR analysis using 1,4-dioxane as an internal standard based on an aryl substituent of triarylbismuth.\\n\\n\\n\\nride (CsF) as a base (entries 3\\xad5). As the result of a further screening of amine ligands, the combined use of 2,2'-biquinoline with CsF was found to provide a considerably good transformation to the 4-methoxybiphenyl (entry 11). It was finally obtained in 68% yield by increasing the amount of 2,2'-biquinoline from 10 mol% to 20 mol% and 4-iodoanisole from 3.3 equiv. to 6.0 equiv. of triphenylbismuth and that is 2.0 equiv. of each phenyl substituent of triphenylbismuth (entry 13).\\n\\nNext, the general applicability of the cross-coupling reaction using electron-sufficient halobenzenes under the optimized reaction conditions was evaluated (Table 3, entries 12 and 13); the amount of iodoarenes was increased from 3.3 equiv. to 6.0 equiv. in the case of low yield of the cross-coupling product. While the 4- and 3-iodotoluene derivatives uneventfully reacted (Table 4, entries 1 and 2), the phenylation of 2-iodotoluene with triphenylbismuth afforded 2-methylbiphenyl in a moderate yield (52%) because of the steric hindrance around the coupling site (entry 3). Although the cross-coupling reaction of the iodoanisoles indicated a somewhat lower efficiency presumably due to the difficult oxidative addition to the palladium metal based on the enhanced electron density on the aromatic nuclei by the strong electron-donating methoxy substituent (entries 4\\xad9), the increase in the amount of iodoanisole to 6.0 equiv. effectively improved the reaction progress to the corresponding\\n\\n\\n\\n2562\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n\\x0cPalladium on Carbon-Catalyzed Cross-Coupling using Triarylbismuths\\n\\n\\n\\nTable 3. Optimization for the cross-coupling reaction of tri- Table 4. 10% Pd/C-catalyzed cross-coupling using 2,2'-biqui-\\n\\n\\n\\nphenylbismuth with 4-iodoanisole.\\n\\n\\n\\nnoline as a ligand.\\n\\n\\n\\nEntry Base\\n\\n\\n\\nLigand\\n\\n\\n\\nYield [%][a]\\n\\n\\n\\n1[b] 2 3 4 5 6 7 8 9 10 11 12[c] 13[c,d]\\n\\n\\n\\nNa3PO4·12 H2O DABCO\\n\\n\\n\\n20\\n\\n\\n\\nNa3PO4·12 H2O DABCO\\n\\n\\n\\n30\\n\\n\\n\\nK2CO3\\n\\n\\n\\nDABCO\\n\\n\\n\\n36\\n\\n\\n\\nKF\\n\\n\\n\\nDABCO\\n\\n\\n\\n38\\n\\n\\n\\nCsF\\n\\n\\n\\nDABCO\\n\\n\\n\\n47\\n\\n\\n\\nCsF\\n\\n\\n\\nTMEDA\\n\\n\\n\\n0\\n\\n\\n\\nCsF\\n\\n\\n\\ndiethylenetriamine 14\\n\\n\\n\\nCsF\\n\\n\\n\\n1,10-phenanthroline 12\\n\\n\\n\\nCsF\\n\\n\\n\\n2,2'-bipyridyl\\n\\n\\n\\n32\\n\\n\\n\\nCsF\\n\\n\\n\\na,a',a''-terpyridine 43\\n\\n\\n\\nCsF\\n\\n\\n\\n2,2'-biquinoline\\n\\n\\n\\n49\\n\\n\\n\\nCsF\\n\\n\\n\\n2,2'-biquinoline\\n\\n\\n\\n51\\n\\n\\n\\nCsF\\n\\n\\n\\n2,2'-biquinoline\\n\\n\\n\\n68\\n\\n\\n\\n[a] Determined by GC/MS using n-tridecane as an internal\\n\\nstandard based on a phenyl substituent of triphenylbis-\\n\\nmuth. [b] 120 8C. [c] 20 mol% of 2,2'-biquinoline was used. [d] 6.0 equiv. of 4-iodoanisole were used.\\n\\n\\n\\nmethoxybiphenyls in each case (entries 5, 7, and 9). The phenylation of 2-iodo-m-xylene afforded 2,6-dimethylbiphenyl in 45% yield, this relatively low yield is explained by a steric effect (entry 10). Both triNGTHCUREA(2tolyl)bismuth and tri(2-methoxyphenyl)bismuth could also be used for the cross-coupling with aryl iodides (entries 11 and 12). 4-Acetyliodobenzene as an electron-deficient substrate was successfully cross-coupled with triphenylbismuth to produce 4-acetylbiphenyl in an excellent yield (entry 13), but the phenylation of 4iodonitrobenzene gave only moderate yield because of the competitive and intercurrent homo-coupling of the exceptionally reactive 4-iodonitrobenzene (entry 14).[13] The yield of the cross-coupling product was conclusively improved to 88% by the increase of the iodide to 6.0 equiv. (entry 15). On the contrary, aryl bromides, even bearing an acetyl group on the aryl nucleus, were less reactive and substantial amounts of the unreacted aryl bromides as starting materials remained (entries 16\\xad18).\\n\\nDuring the course of the screening for the crosscoupling between triphenylbismuth and 4-iodoanisole, tetra-n-buthylammonium fluoride trihydrate (TBAF·3 H2O) was found to be a good base to afford the coupling product in 49% yield even without ligands (Table 5, entry 1).[14,15] The 3- and 2-iodoanisoles were also cross-coupled with triphenylbismuth\\n\\n\\n\\nEntry\\n\\n\\n\\nX\\n\\n\\n\\nR1\\n\\n\\n\\nR2\\n\\n\\n\\nYield [%][a]\\n\\n\\n\\n1\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-Me\\n\\n\\n\\n87\\n\\n\\n\\n2\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-Me\\n\\n\\n\\n89\\n\\n\\n\\n3\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-Me\\n\\n\\n\\n52\\n\\n\\n\\n4\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\n51[b]\\n\\n\\n\\n5[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\n62\\n\\n\\n\\n6\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-MeO\\n\\n\\n\\n21\\n\\n\\n\\n7[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-MeO\\n\\n\\n\\n68\\n\\n\\n\\n8\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-MeO\\n\\n\\n\\n35\\n\\n\\n\\n9[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-MeO\\n\\n\\n\\n61\\n\\n\\n\\n10\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2,6-di-Me\\n\\n\\n\\n45\\n\\n\\n\\n11\\n\\n\\n\\nI\\n\\n\\n\\nMeO\\n\\n\\n\\n4-Me\\n\\n\\n\\n64\\n\\n\\n\\n12[c]\\n\\n\\n\\nI\\n\\n\\n\\nMe\\n\\n\\n\\n4-MeO\\n\\n\\n\\n55\\n\\n\\n\\n13\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\n96\\n\\n\\n\\n14\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n15[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n16\\n\\n\\n\\nBr\\n\\n\\n\\nH\\n\\n\\n\\n4-NO2 4-NO2 4-Me\\n\\n\\n\\n48 (53[d]) 88 32\\n\\n\\n\\n17\\n\\n\\n\\nBr\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\n42\\n\\n\\n\\n18\\n\\n\\n\\nBr\\n\\n\\n\\nH\\n\\n\\n\\n4-NO2\\n\\n\\n\\n24\\n\\n\\n\\n[a] Isolated yield base on an aryl substituent of triarylbis-\\n\\nmuth. [b] Determined by GC/MS using n-tridecane as an internal\\n\\nstandard base on an aryl substituent of triarylbismuth. [c] 6.0 equiv. of iodoarene were used. [d] Determined by 1H NMR using 1,4-dioxane as an internal\\n\\nstandard base on an aryl substituent of triarylbismuth.\\n\\n\\n\\n(entries 4 and 6). The increase in the use of 4- and 3iodoanisoles from 3.3 equiv. to 6.0 equiv. of the triarylbismuth derivatives markedly enhanced the reaction efficiency (entries 2, 3, and 5), although the significant effect was not achieved for the reaction of 2-iodoanisole (entry 7). The iodotoluenes were good substrates for the phenylation regardless of the substitution patterns (entries 8\\xad10), and the reaction of 2-iodo-mxylene afforded 2,6-dimethylbiphenyl in 53% yield in spite of the bulkiness of the two methyl groups around the coupling site (entry 11). Furthermore, the iodobenzenes possessing an electron-withdrawing chloro, acetyl, or nitro group at the 4-position of the aromatic nuclei could also undergo the cross-coupling to give the corresponding biphenyls in 58%, 66%, and 62% yields, respectively (entries 12\\xad14). The phenylation of bromoarenes, however, resulted in low yields of the cross-coupling products. These results suggest that the present ligand-free cross-coupling reaction is applicable to the cross-coupling of aryl iodides.\\n\\nA reduced residual property is an important factor of heterogeneous transition metal catalysts. The\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n2563\\n\\n\\n\\n\\x0cUPDATES\\n\\n\\n\\nYasunari Monguchi et al.\\n\\n\\n\\nTable 5. 10% Pd/C-catalyzed ligand-free cross-coupling of triarylbismuth with aryl halides.\\n\\n\\n\\nEntry\\n\\n\\n\\nX\\n\\n\\n\\nR1\\n\\n\\n\\nR2\\n\\n\\n\\nYield [%][a]\\n\\n\\n\\n1\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\n49[b]\\n\\n\\n\\n2[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-MeO\\n\\n\\n\\n58 (69[b])\\n\\n\\n\\n3[c]\\n\\n\\n\\nI\\n\\n\\n\\nMe\\n\\n\\n\\n4-MeO\\n\\n\\n\\n53\\n\\n\\n\\n4\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-MeO\\n\\n\\n\\n53\\n\\n\\n\\n5[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-MeO\\n\\n\\n\\n79\\n\\n\\n\\n6\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-MeO\\n\\n\\n\\n30\\n\\n\\n\\n7[c]\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-MeO\\n\\n\\n\\n38\\n\\n\\n\\n8\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-Me\\n\\n\\n\\n87\\n\\n\\n\\n9\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n3-Me\\n\\n\\n\\n75\\n\\n\\n\\n10\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2-Me\\n\\n\\n\\n68\\n\\n\\n\\n11\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n2,6-di-Me\\n\\n\\n\\n53\\n\\n\\n\\n12\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-Cl\\n\\n\\n\\n58\\n\\n\\n\\n13\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\n66\\n\\n\\n\\n14\\n\\n\\n\\nI\\n\\n\\n\\nH\\n\\n\\n\\n4-NO2\\n\\n\\n\\n62\\n\\n\\n\\n15\\n\\n\\n\\nBr\\n\\n\\n\\nH\\n\\n\\n\\n4-Me\\n\\n\\n\\n39\\n\\n\\n\\n16\\n\\n\\n\\nBr\\n\\n\\n\\nH\\n\\n\\n\\n4-Ac\\n\\n\\n\\n10\\n\\n\\n\\n[a] Isolated yield based on an aryl substituent of triarylbis-\\n\\nmuth. [b] Determined by GC/MS using n-tridecane as an internal\\n\\nstandard based on an aryl substituent of triarylbismuth. [c] 6.0 equiv. of iodoarene were used.\\n\\n\\n\\nusing CsF and 2,2'-biquinoline (entry 2). It is noteworthy that the Pd species was hardly leached into the reaction media (only 0.16% of the total Pd amount of 10% Pd/C) when the reaction was carried out without ligands (entry 3).[16]\\n\\nThe catalyst activity of leached Pd species from 10% Pd/C during the phenylation of 4-iodoanisole in the presence of TBAF·3H2O was next examined by the so-called hot filtration method established by Djakovitch.[17,18] A mixture of 10% Pd/C, triphenylbismuth, 4-iodoanisole, and TBAF·3 H2O in NMP was heated at 140 8C for 15 min and filtered without cooling with a glass fiber filter (< 1 mm) to remove 10% Pd/C. The filtrate containing 35% of the desired 4methoxybiphenyl was futher heated at 140 8C in the absence of 10% Pd/C, and the reaction was monitored for additional 23.5 h (Figure 1). The reaction time-dependently proceeded to give the corresponding biphenyl in 42% yield (23.5 h, &), although the reaction effeicency admittedly decreased compared to the reaction witout filtration (^, standard run). The results\\n\\n\\n\\namount of leached Pd species from 10% Pd/C to the reaction solution during the cross-coupling reaction under three different protocols was measured by inductively coupled plasma-atomic emission spectrometry (ICP-AES) (Table 6). While a non-negligible Pd leaching (17%) was observed in the filtrate after removal of the heterogeneous Pd/C from the reaction mixture of the cross-coupling between the triphenylbismuth and 4-bromonitrobenzene in the presence of Na3PO4·12 H2O and DABCO (entry 1), it was dramatically suppressed in the phenylation of 4-iodoanisole\\n\\n\\n\\nFigure 1. Time-course study on the phenylation of 4-iodoanisole using triphenylbismuth in the filtrate after removal of 10% Pd/C by hot filtration.\\n\\n\\n\\nTable 6. Pd-leaching test for the 10% Pd/C-catalyzed cross-coupling.\\n\\n\\n\\nEntry\\n\\n\\n\\nX\\n\\n\\n\\nR\\n\\n\\n\\nConditions\\n\\n\\n\\n1\\n\\n\\n\\nBr\\n\\n\\n\\nNO2\\n\\n\\n\\nDABCO (10 mol%), Na3PO4·12 H2O (6 equiv.), 120 8C\\n\\n\\n\\n2\\n\\n\\n\\nI\\n\\n\\n\\nMeO\\n\\n\\n\\n2,2'-biquinoline (20 mol%), CsF (6 equiv.), 140 8C\\n\\n\\n\\n3\\n\\n\\n\\nI\\n\\n\\n\\nMeO\\n\\n\\n\\nTBAF·3 H2O (6 equiv.), 140 8C\\n\\n\\n\\n[a] The amount of leached Pd species over the used 10% Pd/C.\\n\\n\\n\\nLeached Pd [%][a]\\n\\n17 2.9 0.16\\n\\n\\n\\n2564\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n\\x0cPalladium on Carbon-Catalyzed Cross-Coupling using Triarylbismuths\\n\\n\\n\\nsuggest that a small amount of leached palladium species could catalyze the present ligand-free cross-coupling between aryl iodides and triphenylbismuth.\\n\\nIn conclusion, three protocols for the Pd/C-catalyzed cross-coupling reaction between aryl halides and triarylbismuths were established. Halobenzenes bearing an electron-withdrawing group on the benzene nucleus effectively underwent the arylation in heated NMP by the combined use of Na3PO4·12 H2O and DABCO as the base and ligand, respectively. Instead, the combination of CsF and 2,2'-biquinoline was effective for the cross-coupling of a variety of aryl iodides. The ligand-free Pd/C-catalyzed arylation of aryl iodides was achieved by the addition of TBAF·3 H2O. The distinctive features of the protocols are the use of the readily available heterogeneous 10% Pd/C, the quite low level of Pd species leaching into the reaction media, and high applicability for the reaction of electron-sufficient aryl iodides.\\n\\nExperimental Section\\n\\nGeneral\\n\\nAll reagents and solvents were obtained from commercial sources and used without further purification. The 10% Pd/ C (dry-type) was obtained from the N.E. Chemcat Co. (Japan). The PLC silica gel 60 F254 (1 mm) (Merk KGaA, Darmstadt) was used for the preparative TLC. The 1H NMR spectra were recorded by a JEOL JEOL JNM AL-400 (400 MHz for 1H) using CDCl3 as the solvent. The chemical shifts (d) are expressed in ppm and are internally referenced (0.00 ppm for tetramethylsilane). The mass spectra (EI) were taken on a JEOL JMS Q1000GC Mk II Quad GC/MS. All products were characterized by their 1H NMR and mass spectra that were identical to those in the literature.\\n\\nTypical Procedure for the Cross-Coupling between Triarylbismuths and Electron-Deficient Halobenzenes (Table 2)\\n\\nA mixture of 10% Pd/C (13.3 mg, 12.5 mmol), triphenylbismuth (110 mg, 250 mmol), 4-bromonitrobenzene (167 mg, 825 mmol), Na3PO4·12 H2O (246 mg, 1.50 mmol) and DABCO (2.8 mg, 25.0 mmol) in NMP (2 mL) was stirred using the Chemistation personal organic synthesizers (EYELA, Tokyo) or Chemist Plaza personal organic synthesizers (Shibata Scientific Technology, Ltd., Tokyo) under an argon atmosphere at 120 8C for 24 h. The mixture was passed through a membrane filter (Millipore Corp., Billerica, MA; Millex-LH, 0.45 mm), and the filter was washed with EtOAc (30 mL). The filtrate was washed with H2O (20 mL  3) and brine (30 mL), dried (MgSO4), filtered, and concentrated under vacuum. The residue was purified by preparative TLC (hexane) to produce 4-nitrobiphenyl as a colorless solid; yield: 112 mg (0.562 mmol, 75%).\\n\\n\\n\\nTypical Procedure for the Cross-Coupling between Triarylbismuths and Electron-Sufficient Halobenzenes (Table 4)\\n\\nA mixture of 10% Pd/C (13.3 mg, 12.5 mmol), triphenylbismuth (110 mg, 250 mmol), 4-iodotoluene (180 mg, 825 mmol), CsF (277 mg, 1.50 mmol), and 2,2'-biquinoline (12.8 mg, 50.0 mmol) in NMP (2 mL) was stirred using the Chemistation personal organic synthesizers (EYELA, Tokyo) or Chemist Plaza personal organic synthesizers (Shibata Scientific Technology, Ltd., Tokyo) under an argon atmosphere at 140 8C for 24 h. The mixture was passed through a membrane filter (Millipore Corp., Billerica, MA; Millex-LH, 0.45 mm), and the filter was washed with EtOAc (30 mL). The filtrate was washed with H2O (20 mL  3) and brine (30 mL), dried (MgSO4), filtered, and concentrated under vacuum. The residue was purified by preparative TLC (hexane) to produce 4-methylbiphenyl as a colorless solid; yield: 109 mg (0.648 mmol, 86%).\\n\\nTypical Procedure for the Ligand-Free CrossCoupling between Triarylbismuths and Halobenzenes (Table 5)\\n\\nA mixture of 10% Pd/C (13.3 mg, 12.5 mmol), triphenylbismuth (110 mg, 250 mmol), 4-iodotoluene (180 mg, 825 mmol), and TBAF·3 H2O (473 mg, 1.50 mmol) in NMP (2 mL) was stirred using the Chemistation personal organic synthesizers (EYELA, Tokyo) or Chemist Plaza personal organic synthesizers (Shibata Scientific Technology, Ltd., Tokyo) under an argon atmosphere at 140 8C for 24 h. The mixture was passed through a membrane filter (Millipore Corp., Billerica, MA; Millex-LH, 0.45 mm), and the filter was washed with EtOAc (30 mL). The filtrate was washed with H2O (20 mL  3) and brine (30 mL), dried (MgSO4), filtered, and concentrated under vacuum. The residue was purified by preparative TLC (hexane) to produce 4-methylbiphenyl as a colorless solid; yield: 110 mg (0.654 mmol, 87%).\\n\\nGeneral Procedure for Pd-Leaching Test for the 10% Pd/C-Catalyzed Cross-Coupling (Table 6)\\n\\nThe reactions were carried out on 19 times the scale described in the typical procedures for Table 2 and Table 5. The reaction was carried out on 20 times the scale described for the typical procedure for Table 4. The mixture in each reaction was filtered through a filter paper (Kiriyama Glass Co., Tokyo: No. 5C, 1 mm) and two kinds of membrane filters (0.45 mm and 0.20 mm, Millipore Corp., Billerica, MA). The filtrate was transferred to a 100-mL volumetric flask, and the flask was filled to the mark with EtOAc. The Pd concentration in the sample was measured by ICP-AES analysis [ICPS-8100 (Shimadzu, Kyoto)] and found to be 42 mg/L for entry 1 (17% of total used 10% Pd/C), 7.6 mg/L for entry 2 (2.9% of total used 10% Pd/C), and 0.4 mg/L for entry 3 (0.16% of total used 10% Pd/C).\\n\\nAcknowledgements\\n\\n\\n\\nWe sincerely thank the N.E. Chemcat Co. for the gift of 10% Pd/C and analysis of the concentration of leached Pd.\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n2565\\n\\n\\n\\n\\x0cUPDATES\\n\\n\\n\\nYasunari Monguchi et al.\\n\\n\\n\\nReferences\\n\\n[1] a) Organobismuth Chemistry, (Eds.: H. Suzuki, Y. Matano), Elsevier, Amsterdam, 2001; b) M. L. N. Rao, S. Shimoda, M. Tanaka, Org. Lett. 1999, 1, 1271\\xad1273; c) S. Shimoda, M. L. N. Rao, M. Tanaka, Organometallics 2000, 19, 931\\xad936; d) M. L. N. Rao, S. Shimoda, O. Yamazaki, M. Tanaka, J. Organomet. Chem. 2002, 659, 117\\xad120; e) S. Shimoda, O. Yamazaki, T. Tanaka, M. L. N. Rao, Y. Suzuki, M. Tanaka, Angew. Chem. 2003, 115, 1889\\xad1892; Angew. Chem. Int. Ed. 2003, 42, 1845\\xad1848; f) O. Yamazaki, T. Tanaka, S. Shimoda, Y. Suzuki, M. Tanaka, Synlett 2004, 1921\\xad1924; g) D. V. Moiseev, Y. B. Malysheva, A. S. Shavyrin, Y. A. Kurskii, A. V. Gushchin, J. Organomet. Chem. 2005, 690, 3652\\xad3663; h) N. Sakurai, T. Mukaiyama, Chem. Lett. 2007, 36, 928\\xad929; i) Y. B. Malysheva, A. V. Gushchin, Y. Mei, Y. Lu, M. Ballauff, S. Proch, R. Kempe, Eur. J. Inorg. Chem. 2008, 379\\xad383; j) A. Gagnon, M. Duplessis, P. Alasbeh, F. BarabØ, J. Org. Chem. 2008, 73, 3604\\xad3607.\\n\\n[2] a) M. L. N. Rao, D. Banerjee, S. Giri, Tetrahedron Lett. 2009, 50, 5757\\xad5761; b) M. L. N. Rao, D. N. Jadhav, P. Dasgupta, Org. Lett. 2010, 12, 2048\\xad2051.\\n\\n[3] a) M. L. N. Rao, O. Yamazaki, S. Shimoda, T. Tanaka, Y. Suzuki, M. Tanaka, Org. Lett. 2001, 3, 4103\\xad4105; b) M. L. N. Rao, D. Banerjee, D. N. Jadhav, Tetrahedron Lett. 2007, 48, 2707\\xad2711; c) M. L. N. Rao, D. Banerjee, D. N. Jadhav, Tetrahedron Lett. 2007, 48, 6644\\xad6647; d) M. L. N. Rao, D. N. Jadhav, D. Banerjee, Tetrahedron 2008, 64, 5762\\xad5772; e) M. L. N. Rao, V. Venkatesh, D. N. Jadhav, Synlett 2009, 2597\\xad2600; f) M. L. N. Rao, D. Banerjee, R. J. Dhanorkar, Tetrahedron Lett. 2010, 51, 6101\\xad6104.\\n\\n[4] a) M. L. N. Rao, V. Venkatesh, D. N. Jadhav, Tetrahedron Lett. 2006, 47, 6975\\xad6978; b) M. L. N. Rao, V. Venkatesh, D. Banerjee, Tetrahedron 2007, 63, 12917\\xad 12926; c) M. L. N. Rao, V. Venkatesh, D. N. Jadhav, J. Organomet. Chem. 2008, 693, 2494\\xad2498; d) M. L. N. Rao, D. N. Jadhav, V. Venkatesh, Tetrahedron Lett. 2009, 50, 4268\\xad4271; e) M. L. N. Rao, S. Giri, D. N. Jadhav, Tetrahedron Lett. 2009, 50, 6133\\xad6138; f) J.-Y. Chen, S.-C. Chen, Y.-J. Tang, C.-Y. Mou, F.-Y. Tsai, J. Mol. Catal. A: Chem. 2009, 307, 88\\xad92.\\n\\n[5] a) L. Yin, J. Liebscher, Chem. Rev. 2007, 107, 133\\xad173; b) M. J. Climent, A. Corma, S. Iborra, Chem. Rev. 2011, 111, 1072\\xad1133.\\n\\n[6] L. Bai, Y. M. Zhang, J.-X. Wang, QSAR Comb. Sci. 2004, 23, 875\\xad882.\\n\\n[7] W.-J. Zhou, K.-H. Wang, J.-X. Wang, D.-F. Huang, Eur. J. Org. Chem. 2010, 416\\xad419.\\n\\n[8] a) S. Nishimura, Handbook of Heterogeneous Catalytic Hydrogenation for Organic Synthesis, Wiley-Interscience, New York, 2001.\\n\\n[9] a) H. Sajiki, T. Kurita, A. Kozaki, G. Zhang, Y. Kitamura, T. Maegawa, K. Hirota, J. Chem. Res. 2004, 593\\xad 595 [Erratum: J. Chem. Res. 2005, 344]; b) H. Sajiki, T. Kurita, A. Kozaki, G. Zhang, Y. Kitamura, T. Maegawa, K. Hirota, Synthesis 2005, 537\\xad542 [Erratum: Synthesis 2005, 852]; c) H. Sajiki, G. Zhang, Y. Kitamura, T. Maegawa, K. Hirota, Synlett 2005, 619\\xad622 [Erratum: Synlett 2005, 1046]; d) T. Maegawa, Y. Kitamura,\\n\\n\\n\\nS. Sako, T. Udzu, A. Sakurai, A. Tanaka, Y. Kobayashi, K. Endo, U. Bora, T. Kurita, A. Kozaki, Y. Monguchi, H. Sajiki, Chem. Eur. J. 2007, 13, 5937\\xad5943; e) Y. Kitamura, A. Sakurai, T. Udzu, T. Maegawa, Y. Monguchi, H. Sajiki, Tetrahedron 2007, 63, 10596\\xad10602; f) Y. Kitamura, S. Sako, T. Udzu, A. Tsutui, T. Maegawa, Y. Monguchi, H. Sajiki, Chem. Commun. 2007, 5069\\xad5071; g) S. Mori, T. Yanase, S. Aoyagi, Y. Monguchi, T. Maegawa, H. Sajiki, Chem. Eur. J. 2008, 14, 6994\\xad6999; h) Y. Kitamura, S. Sako, A. Tsutsui, Y. Monguchi, T. Maegawa, Y. Kitade, H. Sajiki, Adv. Synth. Catal. 2010, 352, 718\\xad730; i) Y. Yabe, T. Maegawa, Y. Monguchi, H. Sajiki, Tetrahedron 2010, 66, 8654\\xad8660; j) T. Yanase, Y. Monguchi, H. Sajiki, RSC Adv. 2012, 2, 590\\xad594. [10] For reviews related to Pd/C-catalyzed cross-coupling reactions, see: a) M. Nishida, T. Tagata, J. Synth. Org. Chem. Jpn. 2004, 62, 737\\xad742; b) M. Seki, J. Synth. Org. Chem. Jpn. 2006, 64, 853\\xad866; c) M. Seki, Synthesis 2006, 2975\\xad2992; d) F.-X. Felpin, T. Ayad, S. Mitra, Eur. J. Org. Chem. 2006, 2679\\xad2690; e) N. T. S. Phan; M. V. D. Sluys; C. W. Jones, Adv. Synth. Catal. 2006, 348, 609\\xad679; f) L. Yin; J. Liebscher, Chem. Rev. 2007, 107, 133\\xad173; g) F.-X. Felpin, E. Fouquet ChemSusChem 2008, 1, 718\\xad724; h) M. Pal, Synlett 2009, 2896\\xad 2912; i) M. Lamblin, L. Nassar-Hardy, J.-C. Hierso, E. Fouquet, F.-X. Felpin, Adv. Synth. Catal. 2010, 352, 33\\xad 79; j) A. Molnµr, Chem. Rev. 2011, 111, 2251\\xad2320; k) L. Djakovitch, N. Batail, M. Genelot, Molecules 2011, 16, 5241\\xad5267; l) L. Djakovitch, K. Koehler, J. G. de Vries, in: Nanoparticles and Catalysis, (Ed.: D. Astruc), Wiley-VCH, Weinheim, 2001, pp 303\\xad348. [11] Use of Pd/C as the catalyst for the cross-couplings of acyl chlorides with triarylbismuths in the presence of triphenylphosphine ligand has been reported, see ref.[4d] [12] The 10% Pd/C (dry-type, k-type) used for the present work is commercially available from N. E. Chemcat Corporation in Japan. The Pd particle size and carbon surface area are approximately 5 nm and 1100 m2 gÀ1, respectively. The 10% Pd/C has been stored in a vial under dry conditions (blue silica gel) and used without any pretreatment. [13] It was confirmed that the generation of significant amounts of 4,4'-dinitrobiphenyl (39%), the simple homo-coupling product of 4-iodonitrobenzene, were observed together with 4-nitrobiphenyl (48%), the regular cross-coupling product by 1H NMR analysis using 1,4-dioxane as an internal standard. [14] The use of wet-type Pd/C is desired from the safety reason due to its low pyrophoric nature. When wettype 10% Pd/C (5 mol%, k-type, N. E. Chemcat Corporation) was used for the ligand-free phenylation of 4-iodoanisole using triphenylbismuth (3.3 equiv.) in the presence of TBAF·3 H2O (6 equiv.), the catalyst activity was almost the same as that of the dry-type Pd/C to afford the corresponding 4-methoxybiphenyl in 41% yield. [15] The dispersion of Pd metal of Pd/C is also an important factor for the progress in the Pd/C-catalyzed cross-coupling reactions. The use of 5% Pd/C (5 mol%, dry-type, k-type, N. E. Chemcat Corporation) instead of 10% Pd/ C (Table 5, entry 1) significantly reduced the reaction efficiency, and the desired 4-methoxybiphenyl was ob-\\n\\n\\n\\n2566\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n\\x0cPalladium on Carbon-Catalyzed Cross-Coupling using Triarylbismuths\\n\\n\\n\\ntained in only 25% yield. Therefore, the lowly dispersed 10% Pd/C is more suitable for the present cross-coupling reaction. [16] No contamination of Pd metal into the product, 4-methoxybiphenyl, after the purification by preparative TLC was confirmed by ICP-AES. [17] For the catalytic activity of leached palladium species of heterogeneous catalysts, see: a) D. E. Bergbreiter, B. Chen, D. Weatherford, J. Mol. Catal. 1992, 74, 409\\xad419; b) S. Jayasree, A. Seayad, R. V. Chaudhari, Chem. Commun. 1999, 1067\\xad1068; c) C. R. LeBlond, F. Zhao, B. M. Bhanage, M. Shirai, M. Arai, Chem. Eur. J. 2000, 6, 843\\xad848; d) A. Biffis, M. Zecca, M. Basato, Eur. J. Inorg. Chem. 2001, 1131\\xad1133; e) I. W. Davies, L. Matty, D. L. Hughes, P. J. Reider, J. Am. Chem. Soc. 2001, 123, 10139\\xad10140; f) D. A. Conlon, B. Pipik, S.\\n\\n\\n\\nFerdinand, C. R. LeBlond, J. R. Sowa Jr, B. Izzo, P. Collins, G.-J. Ho, J. M. Williams, Y.-J. Shi, Y. Sun, Adv. Synth. Catal. 2003, 345, 931\\xad935; g) M. B. Thathagar, J. E. ten Elshof, G. Rothenberg, Angew. Chem. 2006, 118, 2952\\xad2956; Angew. Chem. Int. Ed. 2006, 45, 2886\\xad 2890; h) A. V. Gaikwad, A. Holuigue, M. B. Thathagar, J. E. ten Elshof, G. Rothenberg, Chem. Eur. J. 2007, 13, 6908\\xad6913; i) L. D. Pachón, G. Rothenberg, Appl. Organomet. Chem. 2008, 22, 288\\xad299; also see ref.[9h] [18] For hot and cold filtration method, see: a) L. Joucla, G. Cusati, C. Pinel, L. Djakovitch, Appl. Catal. A: Gen., 2009, 360, 145\\xad153; b) Y. Monguchi, K. Sakai, K. Endo, Y. Fujita, M. Niimura, M. Yoshimura, T. Mizusaki, Y. Sawama, H. Sajiki, ChemCatChem 2012, 4, 546\\xad558; also see ref.[9j]\\n\\n\\n\\nAdv. Synth. Catal. 2012, 354, 2561 \\xad 2567\\n\\n\\n\\n 2012 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\n\\n\\nasc.wiley-vch.de\\n\\n\\n\\n2567\\n\\n\\n\\n\\x0c\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE THAT THE TOKENIZER WILL IGNORE ALL \\N CHARACTERS PRESENT IN THE TEXT FILE.\n",
    "file = open(\"/Users/Jonathan/Desktop/TestFile 10.1002_adsc.201200179.txt\")\n",
    "text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UPDATES\\n\\n\\n\\nDOI: 10.1002/adsc.201200179\\n\\nPalladium on Carbon-Catalyzed Cross-Coupling using Triarylbismuths\\n\\n\\n\\nYasunari Monguchi,a,* Tomohiro Hattori,a Yasuhiro Miyamoto,a Takayoshi Yanase,a Yoshinari '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = text[:200]\n",
    "short_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['updates', 'doi', ':', '10', '.', '100', '##2', '/', 'ads', '##c', '.', '2012', '##00', '##17', '##9', 'pal', '##lad', '##ium', 'on', 'carbon', '-', 'cat', '##aly', '##zed', 'cross', '-', 'coupling', 'using', 'tri', '##ary', '##lb', '##ism', '##uth', '##s', 'ya', '##sun', '##ari', 'mon', '##guchi', ',', 'a', ',', '*', 'tom', '##oh', '##iro', 'hat', '##tori', ',', 'a', 'ya', '##su', '##hiro', 'mi', '##yam', '##oto', ',', 'a', 'tak', '##ayo', '##shi', 'yan', '##ase', ',', 'a', 'yo', '##shin', '##ari']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(short_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14409, 9193, 1024, 2184, 1012, 2531, 2475, 1013, 14997, 2278, 1012, 2262, 8889, 16576, 2683, 14412, 27266, 5007, 2006, 6351, 1011, 4937, 20766, 5422, 2892, 1011, 19780, 2478, 13012, 5649, 20850, 2964, 14317, 2015, 8038, 19729, 8486, 12256, 16918, 1010, 1037, 1010, 1008, 3419, 11631, 9711, 6045, 29469, 1010, 1037, 8038, 6342, 18334, 2771, 14852, 11439, 1010, 1037, 27006, 28852, 6182, 13619, 11022, 1010, 1037, 10930, 17426, 8486]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(short_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14409, 9193, 1024, 2184, 1012, 2531, 2475, 1013, 14997, 2278, 1012, 2262, 8889, 16576, 2683, 14412, 27266, 5007, 2006, 6351, 1011, 4937, 20766, 5422, 2892, 1011, 19780, 2478, 13012, 5649, 20850, 2964, 14317, 2015, 8038, 19729, 8486, 12256, 16918, 1010, 1037, 1010, 1008, 3419, 11631, 9711, 6045, 29469, 1010, 1037, 8038, 6342, 18334, 2771, 14852, 11439, 1010, 1037, 27006, 28852, 6182, 13619, 11022, 1010, 1037, 10930, 17426, 8486, 102]\n"
     ]
    }
   ],
   "source": [
    "#Instead, we can use token.encode to do both\n",
    "print(tokenizer.encode(short_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only need this line one time, to download things.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, need to find a way to chop text into \n",
    "#a list of sentences\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a HUGE command. Takes our full string\n",
    "#And tries to chop it up smartly into sentences.\n",
    "#Sent_tokenize does tokens at a sentence level.\n",
    "tokenized_sentence = tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry Base\n",
      "\n",
      "\n",
      "\n",
      "Ligand Solvent Yield [%][a]\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "KF\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 41\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "CsF\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 24\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "TBAF·3 H2O ­\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "Cs2CO3\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "NaOH\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 26 DMF 26 DMF 22\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "K3PO4\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "Na3PO4·12 H2O ­\n",
      "\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "TMEDA\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 22 DMF 60 DMF 18\n",
      "\n",
      "\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "DMEDA\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 18\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "DABCO\n",
      "\n",
      "\n",
      "\n",
      "­\n",
      "\n",
      "\n",
      "\n",
      "DMF 50\n",
      "\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "\n",
      "\n",
      "Na3PO4·12 H2O DABCO DMF 65\n",
      "\n",
      "\n",
      "\n",
      "12\n",
      "\n",
      "\n",
      "\n",
      "Na3PO4·12 H2O DABCO DMA 78\n",
      "\n",
      "\n",
      "\n",
      "13\n",
      "\n",
      "\n",
      "\n",
      "Na3PO4·12 H2O DABCO DMSO 79\n",
      "\n",
      "\n",
      "\n",
      "14\n",
      "\n",
      "\n",
      "\n",
      "Na3PO4·12 H2O DABCO NMP 80\n",
      "\n",
      "\n",
      "\n",
      "15[b] Na3PO4·12 H2O DABCO toluene 3\n",
      "\n",
      "\n",
      "\n",
      "16[b] Na3PO4·12 H2O DABCO MeCN 4\n",
      "\n",
      "\n",
      "\n",
      "17[b] Na3PO4·12 H2O DABCO MeOH 16\n",
      "\n",
      "\n",
      "\n",
      "18[b] Na3PO4·12 H2O DABCO THF\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "[a] Determined by 1H NMR analysis using 1,4-dioxane as an internal standard based on a phenyl substituent of triphenylbismuth.\n",
      "Max sentence length  241\n"
     ]
    }
   ],
   "source": [
    "#Next, we need to check to see how long our longest\n",
    "#Sentence is in this paper\n",
    "max_len = 0\n",
    "for sentences in tokenized_sentence:\n",
    "    #Tokenize and add [CLS] and [SEP] tokens BERT needs\n",
    "    input_ids = tokenizer.encode(sentences, add_special_tokens = True)\n",
    "    #Update the max length\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if max_len == 286:\n",
    "        max_len = 0\n",
    "        print(sentences)\n",
    "print(\"Max sentence length \", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably do something to sanitize the sentences being put into the tokenizer. Maybe we can update each sentence with the .remove of the /n, /r components? That would lower the number of word inputs we would need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, so max length of 286. Lets buffer\n",
    "#That a bit, in case. Set it to 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer.encode_plus function combines multiple steps for us:\n",
    "\n",
    "Split the sentence into tokens.\n",
    "Add the special [CLS] and [SEP] tokens.\n",
    "Map the tokens to their IDs.\n",
    "Pad or truncate all sentences to the same length.\n",
    "Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
    "The first four features are in tokenizer.encode, but I'm using tokenizer.encode_plus to get the fifth item (attention masks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we need to build a toy labelled dataset here, with toy labels\n",
    "\n",
    "Label list = oxidant, catalyst, reagent, product, solvent, temperature, ligand, acid, base, conversion, yield, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a stupid rule for labling, and make fake labels\n",
    "for sentences in tokenized_sentence:\n",
    "    if len(sentences) > 150:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.5]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show our procedurally generated labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is ripped off from tutorial. Note that we will want to modify it to have a word and character level characterization, rather than a sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  [11]\n",
      "\n",
      "When 4-bromonitrobenzene was stirred with triphenylbismuth in the presence of 10% Pd/C (5 mol%)[12] and potassium fluoride (KF) (2 equiv.\n",
      "Tokenized IDs:  tensor([  101,  1031,  2340,  1033,  2043,  1018,  1011, 22953,  8202,  4183,\n",
      "         3217, 10609, 10431,  2063,  2001, 13551,  2007,  4440, 10222,  8516,\n",
      "        18477, 28120,  2232,  1999,  1996,  3739,  1997,  2184,  1003, 22851,\n",
      "         1013,  1039,  1006,  1019,  9587,  2140,  1003,  1007,  1031,  2260,\n",
      "         1033,  1998, 18044, 19857, 10050,  3207,  1006,  1047,  2546,  1007,\n",
      "         1006,  1016,  1041, 15549,  2615,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all and map tokens to word ids\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentences in tokenized_sentence:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentences,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 400,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt' #Get tensors back as pytorch\n",
    "    )\n",
    "    #Build an attention mask and the input ids of each item\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#Make lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "#We don't have labels made up yet, but they'd need to be tensors\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(\"Original Sentence: \", tokenized_sentence[12])\n",
    "print(\"Tokenized IDs: \", input_ids[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  270 training samples\n",
      "   31 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a quick dataloader, with a set batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a batch for the DataLoader to make\n",
    "batch_size = 32\n",
    "\n",
    "#Create dataloaders for train\n",
    "#Note that the samples will be pulled in\n",
    "#In a random sequence.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    ")\n",
    "\n",
    "#It doesn't matter what order for validation\n",
    "#so we'll just pull all samples sequentially\n",
    "validation_dataloader = DataLoader(\n",
    "                val_dataset,\n",
    "                sampler = SequentialSampler(val_dataset),\n",
    "                batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now we have everything we need except a model. Let's get into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc68c69ba74443ef88a39cbd2490add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c2e9bdc684cd2afd1ff6249485079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False, #Whether model returns attention weights\n",
    "        output_hidden_states = False,#Whether model returns all hidden states\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tell pytorch to run this on the cpu\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n",
    "\n",
    "Batch size: 16, 32\n",
    "Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "Number of epochs: 2, 3, 4\n",
    "We chose:\n",
    "\n",
    "Batch size: 32 (set when creating our DataLoaders)\n",
    "Learning rate: 2e-5\n",
    "Epochs: 4 (we'll see that this is probably too many...)\n",
    "\n",
    "The epsilon parameter eps = 1e-8 is \"a very small number to prevent any division by zero in the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! That's literally all we need, now we just need to right the training functions and validation functions to get it all right, and make sure it's good. First, some helper functions to make things look nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tell torch what to run the model on\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epcoh took: 0:18:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.47\n",
      "  Validation took: 0:00:52\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:20:21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.40\n",
      "  Validation took: 0:00:47\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:17:04\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.32\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:16:40\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.27\n",
      "  Validation took: 0:00:39\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:15:05 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll eventually need to remove the validation step option. We're only using validation steps to determine just how many epochs we should be training over. \n",
    "\n",
    "We should also, for high quality performance, do some quick hyperparameter optimization, bare minimum going through the various suggested parameters in the Adam paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, we need to prepare testing data, and get it organize correctly and loaded into a test dataloader. I'll just copy the code from the demo here. We don't currently have any testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the CoLA benchmark is measured using the \"Matthews correlation coefficient\" (MCC).\n",
    "\n",
    "We use MCC here because the classes are imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll combine the results for all of the batches and calculate our final MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving & Loading Fine-Tuned Model\n",
    "This first cell (taken from run_glue.py here) writes the model and tokenizer out to disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will load the model back from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
