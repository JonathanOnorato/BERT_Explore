{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in some data\n",
    "save_path = '/Users/Jonathan/Desktop/LabeledChemEData/Labeled_Sheets/'\n",
    "#Do we have something that allows us to fill empty cells with \"\" or something else? Maybe set any NaNs to 0? That would imply\n",
    "#A null label. \n",
    "df = pd.read_excel(save_path+\"Carbon_0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>BESIO</th>\n",
       "      <th>entity</th>\n",
       "      <th>mol_class</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>name.1</th>\n",
       "      <th>tokens.1</th>\n",
       "      <th>BESIO.1</th>\n",
       "      <th>...</th>\n",
       "      <th>tokens.48</th>\n",
       "      <th>BESIO.48</th>\n",
       "      <th>entity.48</th>\n",
       "      <th>mol_class.48</th>\n",
       "      <th>Unnamed: 294</th>\n",
       "      <th>name.49</th>\n",
       "      <th>tokens.49</th>\n",
       "      <th>BESIO.49</th>\n",
       "      <th>entity.49</th>\n",
       "      <th>mol_class.49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>In</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>photoelectron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>interaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>spectroscopy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2010.02.003</td>\n",
       "      <td>between</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2013.12.061</td>\n",
       "      <td>(XPS)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2015.08.007</td>\n",
       "      <td>Ltd.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>has</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>The</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydrogels</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                          name       tokens BESIO entity  \\\n",
       "0             0                         Jon O           In   NaN    NaN   \n",
       "1             1                          2010          the   NaN    NaN   \n",
       "2             2                           250  interaction   NaN    NaN   \n",
       "3             3  10.1016/j.carbon.2010.02.003      between   NaN    NaN   \n",
       "4             4                           NaN          gas   NaN    NaN   \n",
       "..          ...                           ...          ...   ...    ...   \n",
       "349         349                           NaN          NaN   NaN    NaN   \n",
       "350         350                           NaN          NaN   NaN    NaN   \n",
       "351         351                           NaN          NaN   NaN    NaN   \n",
       "352         352                           NaN          NaN   NaN    NaN   \n",
       "353         353                           NaN          NaN   NaN    NaN   \n",
       "\n",
       "    mol_class  Unnamed: 6                        name.1       tokens.1  \\\n",
       "0         NaN           0                         Jon O          X-ray   \n",
       "1         NaN           1                          2014  photoelectron   \n",
       "2         NaN           2                           114   spectroscopy   \n",
       "3         NaN           3  10.1016/j.carbon.2013.12.061          (XPS)   \n",
       "4         NaN           4                           NaN            has   \n",
       "..        ...         ...                           ...            ...   \n",
       "349       NaN         349                           NaN            NaN   \n",
       "350       NaN         350                           NaN            NaN   \n",
       "351       NaN         351                           NaN            NaN   \n",
       "352       NaN         352                           NaN            NaN   \n",
       "353       NaN         353                           NaN            NaN   \n",
       "\n",
       "    BESIO.1  ... tokens.48 BESIO.48  entity.48 mol_class.48 Unnamed: 294  \\\n",
       "0       NaN  ...         ©      NaN        NaN          NaN            0   \n",
       "1       NaN  ...      2020      NaN        NaN          NaN            1   \n",
       "2       NaN  ...  Elsevier      NaN        NaN          NaN            2   \n",
       "3       NaN  ...       Ltd      NaN        NaN          NaN            3   \n",
       "4       NaN  ...       The      NaN        NaN          NaN            4   \n",
       "..      ...  ...       ...      ...        ...          ...          ...   \n",
       "349     NaN  ...       NaN      NaN        NaN          NaN          349   \n",
       "350     NaN  ...       NaN      NaN        NaN          NaN          350   \n",
       "351     NaN  ...       NaN      NaN        NaN          NaN          351   \n",
       "352     NaN  ...       NaN      NaN        NaN          NaN          352   \n",
       "353     NaN  ...       NaN      NaN        NaN          NaN          353   \n",
       "\n",
       "                          name.49  tokens.49 BESIO.49  entity.49 mol_class.49  \n",
       "0                           Jon O          ©      NaN        NaN          NaN  \n",
       "1                            2015       2015      NaN        NaN          NaN  \n",
       "2                             104   Elsevier      NaN        NaN          NaN  \n",
       "3    10.1016/j.carbon.2015.08.007       Ltd.      NaN        NaN          NaN  \n",
       "4                             NaN  Hydrogels      NaN        NaN          NaN  \n",
       "..                            ...        ...      ...        ...          ...  \n",
       "349                           NaN        NaN      NaN        NaN          NaN  \n",
       "350                           NaN        NaN      NaN        NaN          NaN  \n",
       "351                           NaN        NaN      NaN        NaN          NaN  \n",
       "352                           NaN        NaN      NaN        NaN          NaN  \n",
       "353                           NaN        NaN      NaN        NaN          NaN  \n",
       "\n",
       "[354 rows x 300 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to update extract_xy_ to pull out just the x values (ie, tokens), and the y values as a smashed together BESIO. He used NLTK sent_tokenize, so we are in the same place, and we can just go back to having sentences or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy_(df):\n",
    "    \"\"\"\n",
    "    This method extracts and correctly aranges the NER training x-values (tokens)\n",
    "    and y-values (BESIO labels) from a pandas dataframe containing labeled NER\n",
    "    data\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas DataFrame, required): Dataframe loaded via pd.read_excel() on\n",
    "            a labeled NER dataset\n",
    "\n",
    "        endings_dict (dictionary, required): Dictionary containing the indicies\n",
    "            where each sentence in each line of tokens ends.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples, containing the x,y pairs\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    columns = df.columns\n",
    "    new_df = pd.DataFrame()\n",
    "    all_tokens = []             # will contain lists of tokens, one for each text\n",
    "\n",
    "    besio = []\n",
    "    mol = []\n",
    "    IorO = []\n",
    "        \n",
    "    for idx, column in enumerate(columns):\n",
    "        # find every column that starts with 'name'\n",
    "        if column.startswith('name'):\n",
    "\n",
    "            # check if the entry in 'name' cell is a str\n",
    "            if isinstance(df[column][0], str):\n",
    "                tokens = df[columns[idx + 1]].values\n",
    "                all_tokens.append(tokens)\n",
    "                #If data is labeled, we should collect them into lists.\n",
    "#                 if nextcol.startswith('BESIO'):\n",
    "#                     labels = df[nextcol].replace(np.nan, 'O')\n",
    "#                 else:\n",
    "#                     labels = df[nextcol].replace(np.nan, '')\n",
    "                df[columns[idx+2]].replace(np.nan, 'O', inplace = True)\n",
    "                besio.append(df[columns[idx+2]].values)\n",
    "                df[columns[idx+3]].replace(np.nan, '', inplace = True)\n",
    "                mol.append(df[columns[idx+3]].values)\n",
    "                df[columns[idx+4]].replace(np.nan, '', inplace = True)\n",
    "                IorO.append(df[columns[idx+4]].values)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    label_values = []\n",
    "   # print(len(besio))\n",
    "    while i < len(besio):\n",
    "        label_values.append([])\n",
    "       # print(len(besio[i]))\n",
    "       # print(range(len(besio[i])))\n",
    "        for j in range(len(besio[i])):\n",
    "            if besio[i][j] == 'O':\n",
    "                label_values[i].append(besio[i][j])\n",
    "            else:\n",
    "                label_values[i].append(besio[i][j]+'-'+mol[i][j]+'-'+IorO[i][j])\n",
    "        i += 1   \n",
    "    return all_tokens, label_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels = extract_xy_(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'E-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'E-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'E-PRO-',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'E-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'I-PRO-',\n",
       " 'I-PRO-',\n",
       " 'I-PRO-',\n",
       " 'E-PRO-',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRO-',\n",
       " 'E-PRO-',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-I',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'B-MOL-O',\n",
       " 'E-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'B-MOL-O',\n",
       " 'I-MOL-O',\n",
       " 'E-MOL-O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-MOL-O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'S-PRO-',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X-ray', 'photoelectron', 'spectroscopy', '(XPS)', 'has', 'been',\n",
       "       'commonly', 'used', 'to', 'determine', 'the',\n",
       "       'nitrogen-containing', 'functional', 'groups', 'of', 'graphene.',\n",
       "       'However,', 'reported', 'assignments', 'of', 'C1s', 'shifts', 'of',\n",
       "       'nitrogen-containing', 'functional', 'groups', 'are', 'unclear.',\n",
       "       'Most', 'works', 'discuss', 'peak', 'shifts', 'of', 'only', 'N1s',\n",
       "       'spectra', 'and', 'C1s', 'shifts', 'and', 'the', 'full', 'width',\n",
       "       'at', 'half', 'maximum', '(FWHM)', 'are', 'excluded.', 'Thus,',\n",
       "       'peak', 'shifts', 'and', 'FWHMs', 'of', 'C1s', 'and', 'N1s', 'XPS',\n",
       "       'spectra', 'of', 'graphene', 'with', 'nitrogen-containing',\n",
       "       'functional', 'groups', 'such', 'as', 'pyridinic,',\n",
       "       'phenanthroline-like,', 'sp2C-NH2,', 'sp', '3C-NH2,', 'pyrrolic,',\n",
       "       'imine,', 'pyridazine-like,', 'pyrazole-like,', 'sp2C-CN,',\n",
       "       'sp3C-CN,', 'and', 'valley', 'quaternary', 'nitrogen', '(Q-N)',\n",
       "       'on', 'edges', 'and', 'sp3C-NH2,', 'center', 'amine,', 'and',\n",
       "       'center', 'Q-N', 'in', 'the', 'basal', 'plane', 'were',\n",
       "       'simulated', 'using', 'density', 'functional', 'theory',\n",
       "       'calculation.', 'Main', 'peaks', 'of', 'C1s', 'spectra', 'were',\n",
       "       'shifted', 'positively', 'and', 'negatively', 'by', 'the',\n",
       "       'electron-withdrawing', 'and', 'electron-donating', 'functional',\n",
       "       'groups,', 'respectively.', 'FWHMs', 'of', 'the', 'main', 'peaks',\n",
       "       'of', 'C1s', 'spectra', 'were', 'influenced', 'by', 'mainly',\n",
       "       'electron-withdrawing', 'functional', 'groups', 'on', 'edges',\n",
       "       'and', 'most', 'functional', 'groups', 'in', 'the', 'basal',\n",
       "       'plane.', 'Sp2C-NH2', 'on', 'zigzag', 'edges', 'is', 'suggested',\n",
       "       'as', 'a', 'reference', 'functional', 'group', 'to', 'adjust',\n",
       "       'the', 'N1s', 'spectra', 'because', 'influence', 'of', 'the',\n",
       "       'functional', 'group', 'on', 'the', 'shift', 'of', 'main', 'peak',\n",
       "       'and', 'the', 'FWHM', 'of', 'C1s', 'spectrum', 'was', 'small.',\n",
       "       '©', 2013, 'Elsevier', 'Ltd.', 'All', 'rights', 'reserved', nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have two lists. Both lists are structured so that each entry represents a unique paper (ie, tokens[1] is a whole paper). We need to now chop it down so that all the extra entries at the end are removed, and the list tokens[1] is only as long as there are words in that paper. \n",
    "\n",
    "Once we have done that for each entry, we will need to build a looping/wrapper function that will read every excel sheet in a directory. It'd be ideal if that looping function would append each new list to the previous list, so we could end up with every single labeled paper in a single set of two lists. \n",
    "\n",
    "After we have that function built, the next step is to try to regenerate our sentence-split structure. First step is to make each paper back into a single string. We'll do this by making a homemade inverse .split() function, which means we'll add each item in the tokens[1] list together with a single whitespace between them. Example is in the case ['The', 'dog', 'ran.'] we would want to regen the original string of ['The dog ran.']. We could do that by doing original_string += (token[1][i]+ ' '). Once we have made each paper in the list back into individual strings, we'll chop each string into individual sentences by using NLTK. \n",
    "\n",
    "From that point, it's more standard BERT. We'll use BERT's tokenizer. We'll need to make sure we hand-extend each label to match the tokenization done by the BERT tokenizer so we don't have length mismatches (a la: https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/ function tokenize_and_preserve_labels) and then we'll send it to a dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper function that loops all excel spreadsheets and truncates each entry in each spreadsheet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
