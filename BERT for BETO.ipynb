{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in some data\n",
    "save_path = '/Users/Jonathan/Desktop/LabeledChemEData/Labeled_Sheets/'\n",
    "#Do we have something that allows us to fill empty cells with \"\" or something else? Maybe set any NaNs to 0? That would imply\n",
    "#A null label. \n",
    "df = pd.read_excel(save_path+\"Carbon_0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>BESIO</th>\n",
       "      <th>entity</th>\n",
       "      <th>mol_class</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>name.1</th>\n",
       "      <th>tokens.1</th>\n",
       "      <th>BESIO.1</th>\n",
       "      <th>...</th>\n",
       "      <th>tokens.48</th>\n",
       "      <th>BESIO.48</th>\n",
       "      <th>entity.48</th>\n",
       "      <th>mol_class.48</th>\n",
       "      <th>Unnamed: 294</th>\n",
       "      <th>name.49</th>\n",
       "      <th>tokens.49</th>\n",
       "      <th>BESIO.49</th>\n",
       "      <th>entity.49</th>\n",
       "      <th>mol_class.49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>In</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Jon O</td>\n",
       "      <td>©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>photoelectron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>interaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>spectroscopy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2010.02.003</td>\n",
       "      <td>between</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2013.12.061</td>\n",
       "      <td>(XPS)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.carbon.2015.08.007</td>\n",
       "      <td>Ltd.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>has</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>The</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydrogels</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                          name       tokens BESIO entity  \\\n",
       "0             0                         Jon O           In   NaN    NaN   \n",
       "1             1                          2010          the   NaN    NaN   \n",
       "2             2                           250  interaction   NaN    NaN   \n",
       "3             3  10.1016/j.carbon.2010.02.003      between   NaN    NaN   \n",
       "4             4                           NaN          gas   NaN    NaN   \n",
       "..          ...                           ...          ...   ...    ...   \n",
       "349         349                           NaN          NaN   NaN    NaN   \n",
       "350         350                           NaN          NaN   NaN    NaN   \n",
       "351         351                           NaN          NaN   NaN    NaN   \n",
       "352         352                           NaN          NaN   NaN    NaN   \n",
       "353         353                           NaN          NaN   NaN    NaN   \n",
       "\n",
       "    mol_class  Unnamed: 6                        name.1       tokens.1  \\\n",
       "0         NaN           0                         Jon O          X-ray   \n",
       "1         NaN           1                          2014  photoelectron   \n",
       "2         NaN           2                           114   spectroscopy   \n",
       "3         NaN           3  10.1016/j.carbon.2013.12.061          (XPS)   \n",
       "4         NaN           4                           NaN            has   \n",
       "..        ...         ...                           ...            ...   \n",
       "349       NaN         349                           NaN            NaN   \n",
       "350       NaN         350                           NaN            NaN   \n",
       "351       NaN         351                           NaN            NaN   \n",
       "352       NaN         352                           NaN            NaN   \n",
       "353       NaN         353                           NaN            NaN   \n",
       "\n",
       "    BESIO.1  ... tokens.48 BESIO.48  entity.48 mol_class.48 Unnamed: 294  \\\n",
       "0       NaN  ...         ©      NaN        NaN          NaN            0   \n",
       "1       NaN  ...      2020      NaN        NaN          NaN            1   \n",
       "2       NaN  ...  Elsevier      NaN        NaN          NaN            2   \n",
       "3       NaN  ...       Ltd      NaN        NaN          NaN            3   \n",
       "4       NaN  ...       The      NaN        NaN          NaN            4   \n",
       "..      ...  ...       ...      ...        ...          ...          ...   \n",
       "349     NaN  ...       NaN      NaN        NaN          NaN          349   \n",
       "350     NaN  ...       NaN      NaN        NaN          NaN          350   \n",
       "351     NaN  ...       NaN      NaN        NaN          NaN          351   \n",
       "352     NaN  ...       NaN      NaN        NaN          NaN          352   \n",
       "353     NaN  ...       NaN      NaN        NaN          NaN          353   \n",
       "\n",
       "                          name.49  tokens.49 BESIO.49  entity.49 mol_class.49  \n",
       "0                           Jon O          ©      NaN        NaN          NaN  \n",
       "1                            2015       2015      NaN        NaN          NaN  \n",
       "2                             104   Elsevier      NaN        NaN          NaN  \n",
       "3    10.1016/j.carbon.2015.08.007       Ltd.      NaN        NaN          NaN  \n",
       "4                             NaN  Hydrogels      NaN        NaN          NaN  \n",
       "..                            ...        ...      ...        ...          ...  \n",
       "349                           NaN        NaN      NaN        NaN          NaN  \n",
       "350                           NaN        NaN      NaN        NaN          NaN  \n",
       "351                           NaN        NaN      NaN        NaN          NaN  \n",
       "352                           NaN        NaN      NaN        NaN          NaN  \n",
       "353                           NaN        NaN      NaN        NaN          NaN  \n",
       "\n",
       "[354 rows x 300 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to update extract_xy_ to pull out just the x values (ie, tokens), and the y values as a smashed together BESIO. He used NLTK sent_tokenize, so we are in the same place, and we can just go back to having sentences or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy_(df):\n",
    "    \"\"\"\n",
    "    This method extracts and correctly aranges the NER training x-values (tokens)\n",
    "    and y-values (BESIO labels) from a pandas dataframe containing labeled NER\n",
    "    data\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas DataFrame, required): Dataframe loaded via pd.read_excel() on\n",
    "            a labeled NER dataset\n",
    "\n",
    "        endings_dict (dictionary, required): Dictionary containing the indicies\n",
    "            where each sentence in each line of tokens ends.\n",
    "\n",
    "    Returns:\n",
    "        two lists of identical shape. One contains all the words that were tokenized for labeling\n",
    "        and the other contains all the labels of those tokenized words.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    columns = df.columns\n",
    "    new_df = pd.DataFrame()\n",
    "    all_tokens = []\n",
    "    besio = []\n",
    "    mol = []\n",
    "    IorO = []\n",
    "        \n",
    "    for idx, column in enumerate(columns):\n",
    "        # find every column that starts with 'name'\n",
    "        if column.startswith('name'):\n",
    "\n",
    "            # check if the entry in 'name' cell is a str\n",
    "            if isinstance(df[column][0], str):\n",
    "                tokens = df[columns[idx + 1]].values\n",
    "                #find the index where the tokens become NaNs, and chop the token length down to that size. \n",
    "                l = 0\n",
    "                for entries in tokens: \n",
    "                    if type(entries) == str:\n",
    "                        l += 1\n",
    "                all_tokens.append(tokens[:l])\n",
    "                df[columns[idx+2]].replace(np.nan, 'O', inplace = True)\n",
    "                besio.append(df[columns[idx+2]][:l].values)\n",
    "                df[columns[idx+3]].replace(np.nan, '', inplace = True)\n",
    "                mol.append(df[columns[idx+3]][:l].values)\n",
    "                df[columns[idx+4]].replace(np.nan, '', inplace = True)\n",
    "                IorO.append(df[columns[idx+4]][:l].values)\n",
    "\n",
    "    i = 0\n",
    "    label_values = []\n",
    "    while i < len(besio):\n",
    "        label_values.append([])\n",
    "        for j in range(len(besio[i])):\n",
    "            #Strip unintentional whitespace from all entries:\n",
    "            besio[i][j] = besio[i][j].replace(\" \", \"\")\n",
    "            mol[i][j] = mol[i][j].replace(\" \", \"\")\n",
    "            IorO[i][j] = IorO[i][j].replace(\" \", \"\")\n",
    "            \n",
    "            if besio[i][j].upper() == 'O':\n",
    "                label_values[i].append(besio[i][j].upper())\n",
    "            if mol[i][j].upper() == 'PRO':\n",
    "                label_values[i].append(besio[i][j].upper()+'-'+mol[i][j].upper())\n",
    "            if IorO[i][j].upper() == 'I' or IorO[i][j].upper() == 'O':\n",
    "                #The below does not handle cases where BESIO or MOL has errors though...\n",
    "                if mol[i][j].upper() != 'PRO':\n",
    "                    label_values[i].append(besio[i][j].upper()+'-'+mol[i][j].upper()+'-'+IorO[i][j].upper())\n",
    "                else: \n",
    "                    print(\"Weird. This Property is organic or inorganic? LOL\")\n",
    "            #PRIME OPPORTUNITY FOR ERROR HANDLING - IF ANYTHING NOT IN THE ABOVE CATS, SOMETHING IS WRONG\n",
    "            \n",
    "        i += 1   \n",
    "    return all_tokens, label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'S-PRO', 'S-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-MOL-O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-MOL-I', 'O', 'S-MOL-O', 'O', 'O', 'S-MOL-O', 'B-MOL-O', 'E-MOL-O', 'B-MOL-O', 'E-MOL-O', 'O', 'B-MOL-O', 'E-MOL-O', 'O', 'O', 'O', 'O', 'B-PRO', 'E-PRO', 'O', 'O', 'S-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-MOL-I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PRO', 'O', 'S-MOL-I', 'O', 'S-MOL-O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PRO', 'O', 'S-PRO', 'O', 'O', 'O', 'O', 'S-MOL-O', 'S-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(labels[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "PRO\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n",
      "ioro\n"
     ]
    }
   ],
   "source": [
    "tokens, labels = extract_xy_(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have two lists. Both lists are structured so that each entry represents a unique paper (ie, tokens[1] is a whole paper). We need to now chop it down so that all the extra entries at the end are removed, and the list tokens[1] is only as long as there are words in that paper. \n",
    "\n",
    "Once we have done that for each entry, we will need to build a looping/wrapper function that will read every excel sheet in a directory. It'd be ideal if that looping function would append each new list to the previous list, so we could end up with every single labeled paper in a single set of two lists. \n",
    "\n",
    "After we have that function built, the next step is to try to regenerate our sentence-split structure. First step is to make each paper back into a single string. We'll do this by making a homemade inverse .split() function, which means we'll add each item in the tokens[1] list together with a single whitespace between them. Example is in the case ['The', 'dog', 'ran.'] we would want to regen the original string of ['The dog ran.']. We could do that by doing original_string += (token[1][i]+ ' '). Once we have made each paper in the list back into individual strings, we'll chop each string into individual sentences by using NLTK. \n",
    "\n",
    "All of the above is now done!\n",
    "\n",
    "From that point, it's more standard BERT. We'll use BERT's tokenizer. We'll need to make sure we hand-extend each label to match the tokenization done by the BERT tokenizer so we don't have length mismatches (a la: https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/ function tokenize_and_preserve_labels) and then we'll send it to a dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, need another def function that takes in a list of tokens, and a list of labels, and returns them each as a list of strings\n",
    "def tokenized_to_string(token_list):\n",
    "    token_stringlist = []\n",
    "    for paper_tokens in token_list:\n",
    "        paper_string = \"\"\n",
    "        for i in paper_tokens:\n",
    "            #This is basically an 'unsplit' method lol\n",
    "            paper_string += (str(i) + \" \")\n",
    "        token_stringlist.append(paper_string)\n",
    "    return token_stringlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_sheets_to_listed_tokens(directory_url):\n",
    "    \"\"\"This function opens a directory of labeled excel sheets from David's excel sheets and returns the tokens as a list \n",
    "    of strings fully combined on a document level. It returns a list of strings, with each string being a full document.\"\"\"\n",
    "    files = os.listdir(directory_url)\n",
    "    token_list = []\n",
    "    label_list = []\n",
    "    sent_labels = []\n",
    "    for file in files:\n",
    "        df = pd.read_excel(directory_url+file)\n",
    "        token, label = extract_xy_(df)\n",
    "        token_list += (tokenized_to_string(token))\n",
    "        label_list += (label)\n",
    "    #Now we tokenize each paper by sentences using NLTK:\n",
    "    #we will also restructure labels to be ordered by sentences. \n",
    "    for i in range(len(token_list)):\n",
    "        sentences = tokenize.sent_tokenize(token_list[i])\n",
    "        token_list[i] = sentences\n",
    "        short_term_labels = []\n",
    "        for j in range(len(token_list[i])):    \n",
    "            length = len(token_list[i][j].split())\n",
    "            short_list = label_list[i][:length]\n",
    "            short_term_labels.append(short_list)\n",
    "            del(label_list[i][:length])\n",
    "        sent_labels.append(short_term_labels)\n",
    "    return token_list, sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weird. This Property is organic or inorganic? LOL\n",
      "Weird. This Property is organic or inorganic? LOL\n"
     ]
    }
   ],
   "source": [
    "list_o_tokens, list_o_labels = labeled_sheets_to_listed_tokens(dir_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In this work we study the carbonization resulting from an intumescence phenomenon of the fire-retardant formulation: ethylene terpolymer-ammonium polyphosphate/pentaerythritol.', 'Characterisation of the intumescent coating is carried out using infrared spectroscopy and MAS-NMR of the solid state.', 'The study shows the formation of organic phosphocarbonaceous esters, which limits depolymerisation or the evolution of gaseous hydrocarbons.', 'These esters prevent the development of condensed polyaromatic structures.', 'These latter are bridged by the polyethylenic links, which provide the mechanical properties of interest in the protective shield.', '©']\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PRO', 'O', 'B-MOL-O', 'I-MOL-O', 'E-MOL-O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOL-O', 'E-MOL-O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'E-PRO'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-MOL-O', 'E-MOL-O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O']]\n"
     ]
    }
   ],
   "source": [
    "print(list_o_tokens[5])\n",
    "print(list_o_labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_encoded_length(token_list):\n",
    "    max_len = 0\n",
    "    for papers in token_list:\n",
    "        for sentences in papers:\n",
    "            input_ids = tokenizer.encode(sentences, add_special_characters = True)\n",
    "            max_len = max(max_len, len(input_ids))\n",
    "            \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the longer the dataset input, the worse performance is going to get. It turns out the performance of BERT scales quadratically with feature size. Esp. when we only have like 17 sentences greater than 150 words, it seems insane to use so much extra dimensionality. Instead, what we'll do is we'll chunk up sentences to a specific length. BUT, in order to get context, we need to have overlap of each chunked sequence, though it's not clear exactly how much overlap we will need, as this is kinda non-standard. My thought is if we limit max input length to 150, we could then chunk stuff up into 150-length inputs, wherein there are like 25 input overlaps on either side of the sentence.  \n",
    "\n",
    "Alternatively, we could just delete any sentences that are greater than a certain size. To begin with, we will just delete sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we chop all sentences with greater than 150 words, we lose 1.4% of our sentences.\n",
    "If we chop all sentences with greater than 160 words, \n",
    "we lose 1.24%\n",
    "If we chop all with greater that 175, we lose 0.78%\n",
    "If we chop with greater than 200, we lose 0.38% (5 sentences)\n",
    "\n",
    "\n",
    "We'll use 150 at first, because we want to try to train locally if at all possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sentences(token_list, label_list, print_pop = False, max_length = 150):\n",
    "    i = 0\n",
    "    #lets use pop to remove these long sentences\n",
    "    while i < len(token_list):\n",
    "        list_of_j = []\n",
    "        j = 0\n",
    "        while j < len(token_list[i]):\n",
    "            input_ids = tokenizer.encode(token_list[i][j], add_special_tokens = True)\n",
    "            if len(input_ids) > 150:\n",
    "                print(\"Found item length: \" + str(len(input_ids)))\n",
    "                list_of_j.append(j)\n",
    "            j += 1\n",
    "        k = len(list_of_j)-1\n",
    "        #Gotta count backwards so we don't disturb the list structure\n",
    "        while k > -1:\n",
    "            if print_pop:\n",
    "                print(token_list[i].pop(list_of_j[k]))\n",
    "                print(label_list[i].pop(list_of_j[k]))\n",
    "            else:\n",
    "                token_list[i].pop(list_of_j[k])\n",
    "                label_list[i].pop(list_of_j[k])\n",
    "            k = k - 1\n",
    "        i += 1 \n",
    "    return token_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found item length: 181\n",
      "Found item length: 189\n",
      "Found item length: 172\n",
      "Found item length: 270\n",
      "Found item length: 178\n",
      "Found item length: 191\n",
      "Found item length: 171\n",
      "Found item length: 151\n",
      "Found item length: 235\n",
      "Found item length: 161\n",
      "Found item length: 509\n",
      "Found item length: 174\n",
      "Found item length: 207\n",
      "Found item length: 248\n",
      "Found item length: 180\n",
      "Found item length: 155\n",
      "Found item length: 165\n",
      "Found item length: 173\n"
     ]
    }
   ],
   "source": [
    "short_sent_list, short_label_list = delete_sentences(list_o_tokens, list_o_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoded_length(short_sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, we'll set our maximum length inside of BERT to be 155, just to be safe. It's probably a bit unnecessary, but for our relatively small dataset, I bet the extra 6D won't be a crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_label_list[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_sent_list[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what I just learned: We can use the tensordataset loader things. We need to change our label list though. The label needs to have an O token for the CLS and SEP tokens, and that O token should also exist for the padding values. We also DO need to increase the length of each individual word label across the tokenized expansion of the word. \n",
    "\n",
    "Finally, once we've done all that, we need to convert the whole label list into individual integer mappings. We should build a label to number mapper, and then a number to label mapper at the same time. Should mostly just be a dictionary and key mapper type thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Long term we should absolutely consider getting rid of S, E labels\n",
    "label_mapping = {'O': 0, \"B-MOL-O\": 1, \"I-MOL-O\": 2, \"E-MOL-O\": 3,\n",
    "                \"S-MOL-O\": 4, \"B-MOL-I\": 5, \"I-MOL-I\": 6, \"E-MOL-I\": 7,\n",
    "                \"S-MOL-I\": 8, \"B-PRO\": 9, \"I-PRO\": 10, \"E-PRO\": 11, \"S-PRO\": 12}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "Original Sentence:  In the interaction between gas molecules with single-walled carbon nanotube (SWCNT) we show that as a result of collisions the gas scattering contributes with an important background signal and should be considered in SWCNT-based gas sensors.\n",
      "Tokenized IDs:  tensor([  101,  1999,  1996,  8290,  2090,  3806, 10737,  2007,  2309,  1011,\n",
      "        17692,  6351, 28991, 28251,  2063,  1006, 25430,  2278,  3372,  1007,\n",
      "         2057,  2265,  2008,  2004,  1037,  2765,  1997, 28820,  1996,  3806,\n",
      "        17501, 16605,  2007,  2019,  2590,  4281,  4742,  1998,  2323,  2022,\n",
      "         2641,  1999, 25430,  2278,  3372,  1011,  2241,  3806, 13907,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 5, 6, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "label_shapes = []\n",
    "count = 0\n",
    "\n",
    "for abstracts, abst_labels in zip(short_sent_list, short_label_list):\n",
    "    for sentences, sent_labels in zip(abstracts, abst_labels):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                                    sentences,\n",
    "                                    add_special_tokens = True,\n",
    "                                    max_length = 155,\n",
    "                                    pad_to_max_length = True,\n",
    "                                    return_attention_mask = True,\n",
    "                                    return_tensors = 'pt'\n",
    "        )\n",
    "        #Ok, now we get our labels based on encoded sizes. \n",
    "        #Make this a standalone function later instead of nesting\n",
    "        #Need to start the CLS token to every label. \n",
    "        #This CLS token should be an 'O', to keep label\n",
    "        #length matching consistent with the tokenized sentence\n",
    "        extend_sent_labels = ['O']\n",
    "        \n",
    "        \n",
    "        #I Bet the problem with things being read in is the difference in length of \n",
    "        #however this chunks sentences versus how the labels were originally split.\n",
    "        \n",
    "        for word, label in zip(sentences, sent_labels):\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            #Find out how many chunks each word gets broken into\n",
    "            n_subwords = len(tokenized_word)\n",
    "            #Extend the length of the labels to match new word length\n",
    "            #Put label in brackets so it knows you want n_subwords entries\n",
    "            #of label, not label times n_subwords\n",
    "            extend_sent_labels.extend([label]*n_subwords)\n",
    "\n",
    "        #This handles increasing the length for padding and sep tokens\n",
    "        #Go all the way to 155. Padding and  SEP should both be O\n",
    "        extend_sent_labels.extend(['O']*(155-len(extend_sent_labels)))    \n",
    "#\n",
    "        #Next step, we need to use the dictionary lookup\n",
    "        #to replace all the values from this list to become \n",
    "        #numbers. for loops to go through the whole list. \n",
    "        #print(sentences)\n",
    "        #print(extend_sent_labels)\n",
    "        for i in range(len(extend_sent_labels)):\n",
    "            if extend_sent_labels[i] in label_mapping.keys():\n",
    "                #Replace the label in extend_set_labels[i] from dict\n",
    "                extend_sent_labels[i] = label_mapping[extend_sent_labels[i]]\n",
    "        #print(extend_sent_labels)\n",
    "        #Then, we make the labels list into a tensor.\n",
    "        #extend_sent_labels = torch.tensor(extend_sent_labels)\n",
    "        test_list = []\n",
    "        test_list.append(extend_sent_labels)\n",
    "        test_list = torch.tensor(test_list)\n",
    "        #Build our attention mask, labels, and input ids of each item.\n",
    "        #print(encoded_dict)\n",
    "        label_shapes.append(test_list)\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        #print(encoded_dict['input_ids'])\n",
    "#         count += 1\n",
    "#         if count == 2:\n",
    "#             print(label_shapes)\n",
    "#             print(input_ids)\n",
    "#             raise TypeError\n",
    "print(len(label_shapes[1]))\n",
    "print(len(input_ids[1]))\n",
    "print(len(attention_masks[1]))\n",
    "\n",
    "#Make lists we just built into tensors\n",
    "input_ids = torch.cat(input_ids, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.cat(label_shapes, dim = 0)\n",
    "\n",
    "print(\"Original Sentence: \", short_sent_list[0][0])\n",
    "print(\"Tokenized IDs: \", input_ids[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1272, 155])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1272, 155])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1272, 155])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now we have a label list, an attention mask, and an input id that are all the same shape, and contain all the data, in full integer form, so everything can be put into a DataLoader, and then we can start thinking about putting everything into the BERT model. Very exciting! Let's start with a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
