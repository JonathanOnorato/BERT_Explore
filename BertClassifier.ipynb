{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-based Classifier with BERT Fine-tuning\n",
    "\n",
    "This notebook is just for a BERT model feeding into a classifier, and fine-tuning the whole stack collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, AdamW, BertConfig\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "# Data Preparation and Cleaning\n",
    "\n",
    "This section focuses on taking in data and cleaning it. Notable functions are those that load in raw data, the option to delete sentences that don't have any non-outside tokens, and the option to delete all sentences above a certain sentence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy(df):\n",
    "    \"\"\"\n",
    "    This method extracts and correctly aranges the NER training x-values (tokens)\n",
    "    and y-values (BESIO labels) from a pandas dataframe containing labeled NER\n",
    "    data\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas DataFrame, required): Dataframe loaded via pd.read_excel() on\n",
    "            a labeled NER dataset\n",
    "\n",
    "    Returns:\n",
    "        two lists of identical shape. One contains all the tokens for labeling\n",
    "        and the other contains all the labels of those tokenized words. Note that these tokens\n",
    "        are not BERT tokens as yet. They still need to be fed into a BERT tokenizer.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    columns = df.columns\n",
    "    new_df = pd.DataFrame()\n",
    "    all_tokens = []\n",
    "    besio = []\n",
    "    mol = []\n",
    "    IorO = []\n",
    "        \n",
    "    for idx, column in enumerate(columns):\n",
    "        # find every column that starts with 'name'\n",
    "        if column.startswith('name'):\n",
    "\n",
    "            # check if the entry in 'name' cell is a str\n",
    "            if isinstance(df[column][0], str):\n",
    "                tokens = df[columns[idx + 1]].values\n",
    "                #find the index where the tokens become NaNs, and chop the token length down to that size. \n",
    "                l = 0\n",
    "                for entries in tokens: \n",
    "                    if type(entries) == str:\n",
    "                        l += 1\n",
    "                all_tokens.append(tokens[:l])\n",
    "                df[columns[idx+2]].replace(np.nan, 'O', inplace = True)\n",
    "                besio.append(df[columns[idx+2]][:l].values)\n",
    "                df[columns[idx+3]].replace(np.nan, '', inplace = True)\n",
    "                mol.append(df[columns[idx+3]][:l].values)\n",
    "                df[columns[idx+4]].replace(np.nan, '', inplace = True)\n",
    "                IorO.append(df[columns[idx+4]][:l].values)\n",
    "\n",
    "    i = 0\n",
    "    label_values = []\n",
    "    while i < len(besio):\n",
    "        label_values.append([])\n",
    "        for j in range(len(besio[i])):\n",
    "            #Strip unintentional whitespace from all entries:\n",
    "            besio[i][j] = besio[i][j].replace(\" \", \"\")\n",
    "            mol[i][j] = mol[i][j].replace(\" \", \"\")\n",
    "            IorO[i][j] = IorO[i][j].replace(\" \", \"\")\n",
    "            \n",
    "            if besio[i][j].upper() == 'O':\n",
    "                label_values[i].append(besio[i][j].upper())\n",
    "            if mol[i][j].upper() == 'PRO':\n",
    "                label_values[i].append(besio[i][j].upper()+'-'+mol[i][j].upper())\n",
    "            if IorO[i][j].upper() == 'I' or IorO[i][j].upper() == 'O':\n",
    "                #The below does not handle cases where BESIO or MOL has errors though...\n",
    "                if mol[i][j].upper() != 'PRO':\n",
    "                    label_values[i].append(besio[i][j].upper()+'-'+mol[i][j].upper()+'-'+IorO[i][j].upper())\n",
    "                else: \n",
    "                    print(\"Weird. This Property is organic or inorganic? LOL\")\n",
    "            #PRIME OPPORTUNITY FOR ERROR HANDLING - IF ANYTHING NOT IN THE ABOVE CATS, SOMETHING IS WRONG\n",
    "            \n",
    "        i += 1   \n",
    "    return all_tokens, label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_string(token_list):\n",
    "    \"\"\"This function is a helper function that takes the data from the labelled sheets, and turns them \n",
    "    from a list format back into a sentence format. Sort of an 'unsplit' method.\"\"\"\n",
    "    token_stringlist = []\n",
    "    for paper_tokens in token_list:\n",
    "        paper_string = \"\"\n",
    "        for i in paper_tokens:\n",
    "            #This is basically an 'unsplit' method lol\n",
    "            paper_string += (str(i) + \" \")\n",
    "        token_stringlist.append(paper_string)\n",
    "    return token_stringlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_sheets_to_listed_tokens(directory_url):\n",
    "    \"\"\"This function opens a directory of labeled excel sheets from NER labeled excel sheets and returns the tokens as a list \n",
    "    of strings fully combined on a document level. It returns a list of strings, with each string being a full document.\"\"\"\n",
    "    files = os.listdir(directory_url)\n",
    "    token_list = []\n",
    "    label_list = []\n",
    "    sent_labels = []\n",
    "    for file in files:\n",
    "        df = pd.read_excel(directory_url+file)\n",
    "        token, label = extract_xy(df)\n",
    "        token_list += (tokenized_to_string(token))\n",
    "        label_list += (label)\n",
    "    #Now we tokenize each paper by sentences using NLTK:\n",
    "    #we will also restructure labels to be ordered by sentences. \n",
    "    for i in range(len(token_list)):\n",
    "        sentences = tokenize.sent_tokenize(token_list[i])\n",
    "        token_list[i] = sentences\n",
    "        short_term_labels = []\n",
    "        for j in range(len(token_list[i])):    \n",
    "            length = len(token_list[i][j].split())\n",
    "            short_list = label_list[i][:length]\n",
    "            short_term_labels.append(short_list)\n",
    "            del(label_list[i][:length])\n",
    "        sent_labels.append(short_term_labels)\n",
    "    return token_list, sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_sentences(token_list, label_list, label_dict):\n",
    "    \"\"\"This function is a form of undersampling, as it is dropping all examples of sentences where only O labels exist. We need to\n",
    "    drop full sentences for undersampling due to the nature of BERT needing full sentences for its context understanding.\"\"\"\n",
    "    labels_list = list(label_dict.keys())\n",
    "    flip = 0\n",
    "    list_of_deletions = []\n",
    "    del_counter = 0\n",
    "    for i in range(len(labels_list)):\n",
    "        if labels_list[i] == 'O':\n",
    "            del(labels_list[i])\n",
    "            flip = 1\n",
    "        if flip == 1:\n",
    "            break\n",
    "    for i in range(len(token_list)):\n",
    "        for j in range(len(token_list[i])):\n",
    "            if any(x in label_list[i][j] for x in labels_list):\n",
    "                pass\n",
    "            else:\n",
    "                list_of_deletions.append([i,j])\n",
    "    print(list_of_deletions)\n",
    "    while len(list_of_deletions) != 0:\n",
    "        i,j = list_of_deletions[-1]\n",
    "        del(label_list[i][j])\n",
    "        del(token_list[i][j])\n",
    "        del(list_of_deletions[-1])\n",
    "        del_counter += 1\n",
    "    print(\"Total deleted sentences = \" + str(del_counter))\n",
    "    return token_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_encoded_length(token_list):\n",
    "    \"\"\"This function takes in a full list of all tokens, and determines the max BERT-encoded length\n",
    "    for any sentence in the corpus, so we can set an appropriate maximum length for our BERT model.\"\"\"\n",
    "    max_len = 0\n",
    "    len_list = []\n",
    "    #Instantiate a tokenizer from the BERT Tokenizer class\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "    \n",
    "    for papers in token_list:\n",
    "        for sentences in papers:\n",
    "            input_ids = tokenizer.encode(sentences, add_special_characters = True)\n",
    "            max_len = max(max_len, len(input_ids))\n",
    "            len_list.append(len(input_ids))\n",
    "            \n",
    "    return max_len, len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sentences(token_list, label_list, print_pop = False, max_length = 150):\n",
    "    \"\"\"This function takes in the list of tokens and labels, and deletes any sentence that has an \n",
    "    encoded tokenized length of greater than max_length. There is an option to print out sentences\n",
    "    that have been deleted by using print_pop = True. \"\"\"\n",
    "    i = 0\n",
    "    #Instantiate a tokenizer from the BERT Tokenizer class\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "    \n",
    "    #lets use pop to remove these long sentences\n",
    "    while i < len(token_list):\n",
    "        list_of_j = []\n",
    "        j = 0\n",
    "        while j < len(token_list[i]):\n",
    "            input_ids = tokenizer.encode(token_list[i][j], add_special_tokens = True)\n",
    "            if len(input_ids) > 150:\n",
    "                print(\"Found item length: \" + str(len(input_ids)))\n",
    "                list_of_j.append(j)\n",
    "            j += 1\n",
    "        k = len(list_of_j)-1\n",
    "        #Gotta count backwards so we don't disturb the list structure\n",
    "        while k > -1:\n",
    "            if print_pop:\n",
    "                print(token_list[i].pop(list_of_j[k]))\n",
    "                print(label_list[i].pop(list_of_j[k]))\n",
    "            else:\n",
    "                token_list[i].pop(list_of_j[k])\n",
    "                label_list[i].pop(list_of_j[k])\n",
    "            k = k - 1\n",
    "        i += 1 \n",
    "    return token_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_lables(list_of_tokens, list_of_labels, encoding_dict, max_sent_length = 155):\n",
    "    \"\"\"This function takes the token list, BERT-tokenizes it, all while maintaining the match between \n",
    "    labels and words. This is important as BERT breaks down full tokens into subwords, so sometimes\n",
    "    a label will need to span multiple subwords to correctly label the situation. This function\n",
    "    also builds the padding tokens needed, padding to a length set by max_sent_length.\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_shapes = []\n",
    "    no_pad_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    #Instantiate a tokenizer from the BERT Tokenizer class\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "\n",
    "    for abstracts, abst_labels in zip(list_of_tokens, list_of_labels):\n",
    "        for sentences, sent_labels in zip(abstracts, abst_labels):\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                        sentences,\n",
    "                                        add_special_tokens = True,\n",
    "                                        max_length = max_sent_length,\n",
    "                                        pad_to_max_length = True,\n",
    "                                        return_attention_mask = True,\n",
    "                                        return_tensors = 'pt'\n",
    "            )\n",
    "            #Ok, now we get our labels based on encoded sizes. \n",
    "            #Make this a standalone function later instead of nesting\n",
    "            #Need to start the CLS token to every label. \n",
    "            #This CLS token should be a int 0, to keep label\n",
    "            #length matching consistent with the tokenized sentence\n",
    "            extend_sent_labels = [0]\n",
    "\n",
    "\n",
    "            #I Bet the problem with things being read in is the difference in length of \n",
    "            #however this chunks sentences versus how the labels were originally split.\n",
    "\n",
    "            for word, label in zip(sentences, sent_labels):\n",
    "                tokenized_word = tokenizer.tokenize(word)\n",
    "                #Find out how many chunks each word gets broken into\n",
    "                n_subwords = len(tokenized_word)\n",
    "                #Extend the length of the labels to match new word length\n",
    "                #Put label in brackets so it knows you want n_subwords entries\n",
    "                #of label, not label times n_subwords\n",
    "                extend_sent_labels.extend([label]*n_subwords)\n",
    "\n",
    "            #In order to know just how much to bias the dataset for each label, we need to know how many of each we have. \n",
    "            no_pad_labels.extend(extend_sent_labels)\n",
    "\n",
    "            #This handles increasing the length for padding and sep tokens\n",
    "            #Go all the way to 155. Padding and  SEP should both be PAD tokens in label form\n",
    "            #Because the key map will switch them to a 0. \n",
    "            extend_sent_labels.extend(['PAD']*(155-len(extend_sent_labels)))    \n",
    "    \n",
    "            #Next step, we need to use the dictionary lookup\n",
    "            #to replace all the values from this list to become \n",
    "            #numbers. for loops to go through the whole list. \n",
    "            for i in range(len(extend_sent_labels)):\n",
    "                if extend_sent_labels[i] in encoding_dict.keys():\n",
    "                    #Replace the label in extend_set_labels[i] from dict\n",
    "                    extend_sent_labels[i] = encoding_dict[extend_sent_labels[i]]\n",
    "            #print(extend_sent_labels)\n",
    "            #Then, we make the labels list into a tensor.\n",
    "            #extend_sent_labels = torch.tensor(extend_sent_labels)\n",
    "            test_list = []\n",
    "            test_list.append(extend_sent_labels)\n",
    "            test_list = torch.tensor(test_list)\n",
    "            #Build our attention mask, labels, and input ids of each item.\n",
    "            label_shapes.append(test_list)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    #Make lists we just built into tensors\n",
    "    input_ids = torch.cat(input_ids, dim = 0)\n",
    "    attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    labels = torch.cat(label_shapes, dim = 0)\n",
    "\n",
    "    print(\"Original Sentence: \", list_of_tokens[0][0])\n",
    "    print(\"Tokenized IDs: \", input_ids[0])\n",
    "    print(\"Extended Labels: \", labels[0])\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the above functions:\n",
    "\n",
    "This next set shows calling all the above functions, and then getting to the point where data is ready to input into a BERT model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weird. This Property is organic or inorganic? LOL\n",
      "Weird. This Property is organic or inorganic? LOL\n"
     ]
    }
   ],
   "source": [
    "#Load all of the data into two lists, one for tokens, one for labels.\n",
    "dir_url = '/Users/Jonathan/Desktop/LabeledChemEData/Labeled_Sheets/'\n",
    "list_o_tokens, list_o_labels = labeled_sheets_to_listed_tokens(dir_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509,\n",
       " [51,\n",
       "  27,\n",
       "  25,\n",
       "  6,\n",
       "  30,\n",
       "  19,\n",
       "  32,\n",
       "  136,\n",
       "  28,\n",
       "  34,\n",
       "  52,\n",
       "  10,\n",
       "  47,\n",
       "  27,\n",
       "  22,\n",
       "  48,\n",
       "  10,\n",
       "  46,\n",
       "  30,\n",
       "  38,\n",
       "  32,\n",
       "  33,\n",
       "  45,\n",
       "  32,\n",
       "  65,\n",
       "  19,\n",
       "  18,\n",
       "  40,\n",
       "  49,\n",
       "  26,\n",
       "  37,\n",
       "  15,\n",
       "  27,\n",
       "  3,\n",
       "  43,\n",
       "  181,\n",
       "  18,\n",
       "  28,\n",
       "  34,\n",
       "  39,\n",
       "  26,\n",
       "  30,\n",
       "  9,\n",
       "  39,\n",
       "  28,\n",
       "  50,\n",
       "  29,\n",
       "  40,\n",
       "  26,\n",
       "  78,\n",
       "  32,\n",
       "  56,\n",
       "  46,\n",
       "  35,\n",
       "  35,\n",
       "  11,\n",
       "  51,\n",
       "  20,\n",
       "  24,\n",
       "  22,\n",
       "  21,\n",
       "  30,\n",
       "  33,\n",
       "  30,\n",
       "  17,\n",
       "  12,\n",
       "  20,\n",
       "  30,\n",
       "  34,\n",
       "  40,\n",
       "  45,\n",
       "  21,\n",
       "  34,\n",
       "  14,\n",
       "  13,\n",
       "  17,\n",
       "  30,\n",
       "  41,\n",
       "  48,\n",
       "  11,\n",
       "  18,\n",
       "  28,\n",
       "  31,\n",
       "  31,\n",
       "  10,\n",
       "  36,\n",
       "  14,\n",
       "  26,\n",
       "  46,\n",
       "  23,\n",
       "  31,\n",
       "  33,\n",
       "  33,\n",
       "  35,\n",
       "  4,\n",
       "  35,\n",
       "  35,\n",
       "  38,\n",
       "  26,\n",
       "  47,\n",
       "  43,\n",
       "  10,\n",
       "  34,\n",
       "  27,\n",
       "  41,\n",
       "  41,\n",
       "  10,\n",
       "  53,\n",
       "  35,\n",
       "  33,\n",
       "  40,\n",
       "  13,\n",
       "  18,\n",
       "  26,\n",
       "  16,\n",
       "  3,\n",
       "  49,\n",
       "  38,\n",
       "  53,\n",
       "  69,\n",
       "  52,\n",
       "  28,\n",
       "  37,\n",
       "  31,\n",
       "  33,\n",
       "  56,\n",
       "  20,\n",
       "  29,\n",
       "  42,\n",
       "  23,\n",
       "  83,\n",
       "  41,\n",
       "  29,\n",
       "  23,\n",
       "  10,\n",
       "  29,\n",
       "  34,\n",
       "  25,\n",
       "  34,\n",
       "  30,\n",
       "  33,\n",
       "  87,\n",
       "  22,\n",
       "  35,\n",
       "  48,\n",
       "  40,\n",
       "  57,\n",
       "  37,\n",
       "  38,\n",
       "  33,\n",
       "  51,\n",
       "  46,\n",
       "  49,\n",
       "  49,\n",
       "  7,\n",
       "  83,\n",
       "  26,\n",
       "  29,\n",
       "  28,\n",
       "  57,\n",
       "  60,\n",
       "  28,\n",
       "  3,\n",
       "  45,\n",
       "  30,\n",
       "  58,\n",
       "  43,\n",
       "  11,\n",
       "  46,\n",
       "  27,\n",
       "  30,\n",
       "  21,\n",
       "  18,\n",
       "  18,\n",
       "  40,\n",
       "  43,\n",
       "  34,\n",
       "  48,\n",
       "  35,\n",
       "  10,\n",
       "  12,\n",
       "  54,\n",
       "  21,\n",
       "  38,\n",
       "  59,\n",
       "  21,\n",
       "  28,\n",
       "  33,\n",
       "  30,\n",
       "  28,\n",
       "  48,\n",
       "  43,\n",
       "  27,\n",
       "  45,\n",
       "  32,\n",
       "  10,\n",
       "  22,\n",
       "  16,\n",
       "  42,\n",
       "  29,\n",
       "  20,\n",
       "  29,\n",
       "  28,\n",
       "  59,\n",
       "  23,\n",
       "  27,\n",
       "  38,\n",
       "  14,\n",
       "  9,\n",
       "  30,\n",
       "  39,\n",
       "  46,\n",
       "  27,\n",
       "  46,\n",
       "  18,\n",
       "  9,\n",
       "  42,\n",
       "  45,\n",
       "  30,\n",
       "  16,\n",
       "  24,\n",
       "  5,\n",
       "  43,\n",
       "  34,\n",
       "  32,\n",
       "  27,\n",
       "  39,\n",
       "  52,\n",
       "  20,\n",
       "  10,\n",
       "  36,\n",
       "  45,\n",
       "  20,\n",
       "  13,\n",
       "  25,\n",
       "  28,\n",
       "  32,\n",
       "  23,\n",
       "  22,\n",
       "  27,\n",
       "  8,\n",
       "  7,\n",
       "  23,\n",
       "  44,\n",
       "  15,\n",
       "  40,\n",
       "  43,\n",
       "  40,\n",
       "  11,\n",
       "  39,\n",
       "  49,\n",
       "  45,\n",
       "  46,\n",
       "  33,\n",
       "  3,\n",
       "  18,\n",
       "  52,\n",
       "  40,\n",
       "  17,\n",
       "  31,\n",
       "  33,\n",
       "  17,\n",
       "  18,\n",
       "  26,\n",
       "  38,\n",
       "  7,\n",
       "  18,\n",
       "  20,\n",
       "  23,\n",
       "  39,\n",
       "  17,\n",
       "  16,\n",
       "  16,\n",
       "  19,\n",
       "  24,\n",
       "  22,\n",
       "  19,\n",
       "  19,\n",
       "  32,\n",
       "  25,\n",
       "  19,\n",
       "  29,\n",
       "  31,\n",
       "  16,\n",
       "  27,\n",
       "  22,\n",
       "  20,\n",
       "  33,\n",
       "  11,\n",
       "  35,\n",
       "  21,\n",
       "  32,\n",
       "  49,\n",
       "  28,\n",
       "  41,\n",
       "  33,\n",
       "  83,\n",
       "  39,\n",
       "  27,\n",
       "  35,\n",
       "  41,\n",
       "  20,\n",
       "  10,\n",
       "  29,\n",
       "  27,\n",
       "  54,\n",
       "  18,\n",
       "  57,\n",
       "  23,\n",
       "  33,\n",
       "  28,\n",
       "  58,\n",
       "  47,\n",
       "  50,\n",
       "  50,\n",
       "  16,\n",
       "  56,\n",
       "  21,\n",
       "  33,\n",
       "  43,\n",
       "  65,\n",
       "  69,\n",
       "  38,\n",
       "  40,\n",
       "  38,\n",
       "  33,\n",
       "  26,\n",
       "  34,\n",
       "  36,\n",
       "  28,\n",
       "  30,\n",
       "  10,\n",
       "  34,\n",
       "  48,\n",
       "  25,\n",
       "  54,\n",
       "  30,\n",
       "  19,\n",
       "  32,\n",
       "  44,\n",
       "  19,\n",
       "  32,\n",
       "  28,\n",
       "  36,\n",
       "  58,\n",
       "  98,\n",
       "  22,\n",
       "  37,\n",
       "  11,\n",
       "  54,\n",
       "  35,\n",
       "  28,\n",
       "  27,\n",
       "  101,\n",
       "  45,\n",
       "  29,\n",
       "  25,\n",
       "  32,\n",
       "  29,\n",
       "  45,\n",
       "  11,\n",
       "  32,\n",
       "  25,\n",
       "  25,\n",
       "  46,\n",
       "  12,\n",
       "  12,\n",
       "  29,\n",
       "  33,\n",
       "  28,\n",
       "  18,\n",
       "  39,\n",
       "  57,\n",
       "  12,\n",
       "  55,\n",
       "  59,\n",
       "  36,\n",
       "  11,\n",
       "  45,\n",
       "  73,\n",
       "  43,\n",
       "  45,\n",
       "  40,\n",
       "  44,\n",
       "  44,\n",
       "  25,\n",
       "  48,\n",
       "  41,\n",
       "  30,\n",
       "  11,\n",
       "  23,\n",
       "  19,\n",
       "  33,\n",
       "  23,\n",
       "  36,\n",
       "  20,\n",
       "  35,\n",
       "  24,\n",
       "  28,\n",
       "  28,\n",
       "  32,\n",
       "  8,\n",
       "  30,\n",
       "  48,\n",
       "  21,\n",
       "  25,\n",
       "  69,\n",
       "  16,\n",
       "  34,\n",
       "  84,\n",
       "  24,\n",
       "  48,\n",
       "  67,\n",
       "  17,\n",
       "  31,\n",
       "  59,\n",
       "  4,\n",
       "  26,\n",
       "  50,\n",
       "  36,\n",
       "  29,\n",
       "  19,\n",
       "  40,\n",
       "  6,\n",
       "  22,\n",
       "  13,\n",
       "  29,\n",
       "  26,\n",
       "  45,\n",
       "  51,\n",
       "  100,\n",
       "  34,\n",
       "  18,\n",
       "  22,\n",
       "  30,\n",
       "  52,\n",
       "  32,\n",
       "  10,\n",
       "  70,\n",
       "  32,\n",
       "  47,\n",
       "  61,\n",
       "  43,\n",
       "  30,\n",
       "  37,\n",
       "  19,\n",
       "  114,\n",
       "  31,\n",
       "  35,\n",
       "  96,\n",
       "  44,\n",
       "  42,\n",
       "  29,\n",
       "  34,\n",
       "  8,\n",
       "  57,\n",
       "  22,\n",
       "  53,\n",
       "  27,\n",
       "  35,\n",
       "  32,\n",
       "  76,\n",
       "  12,\n",
       "  44,\n",
       "  62,\n",
       "  27,\n",
       "  39,\n",
       "  40,\n",
       "  7,\n",
       "  12,\n",
       "  18,\n",
       "  69,\n",
       "  17,\n",
       "  47,\n",
       "  16,\n",
       "  19,\n",
       "  33,\n",
       "  18,\n",
       "  19,\n",
       "  36,\n",
       "  47,\n",
       "  34,\n",
       "  58,\n",
       "  37,\n",
       "  24,\n",
       "  8,\n",
       "  33,\n",
       "  34,\n",
       "  29,\n",
       "  54,\n",
       "  75,\n",
       "  25,\n",
       "  46,\n",
       "  30,\n",
       "  31,\n",
       "  52,\n",
       "  42,\n",
       "  35,\n",
       "  14,\n",
       "  74,\n",
       "  22,\n",
       "  14,\n",
       "  10,\n",
       "  26,\n",
       "  23,\n",
       "  36,\n",
       "  41,\n",
       "  57,\n",
       "  49,\n",
       "  50,\n",
       "  62,\n",
       "  11,\n",
       "  30,\n",
       "  21,\n",
       "  49,\n",
       "  42,\n",
       "  66,\n",
       "  70,\n",
       "  45,\n",
       "  91,\n",
       "  26,\n",
       "  36,\n",
       "  49,\n",
       "  38,\n",
       "  34,\n",
       "  23,\n",
       "  48,\n",
       "  57,\n",
       "  33,\n",
       "  67,\n",
       "  42,\n",
       "  24,\n",
       "  8,\n",
       "  22,\n",
       "  25,\n",
       "  30,\n",
       "  13,\n",
       "  38,\n",
       "  32,\n",
       "  19,\n",
       "  127,\n",
       "  76,\n",
       "  23,\n",
       "  10,\n",
       "  30,\n",
       "  49,\n",
       "  57,\n",
       "  32,\n",
       "  4,\n",
       "  15,\n",
       "  15,\n",
       "  49,\n",
       "  15,\n",
       "  31,\n",
       "  31,\n",
       "  29,\n",
       "  41,\n",
       "  24,\n",
       "  30,\n",
       "  59,\n",
       "  23,\n",
       "  11,\n",
       "  28,\n",
       "  49,\n",
       "  73,\n",
       "  34,\n",
       "  26,\n",
       "  39,\n",
       "  54,\n",
       "  18,\n",
       "  23,\n",
       "  40,\n",
       "  11,\n",
       "  89,\n",
       "  41,\n",
       "  20,\n",
       "  39,\n",
       "  56,\n",
       "  40,\n",
       "  32,\n",
       "  33,\n",
       "  11,\n",
       "  5,\n",
       "  44,\n",
       "  11,\n",
       "  50,\n",
       "  34,\n",
       "  59,\n",
       "  25,\n",
       "  32,\n",
       "  53,\n",
       "  14,\n",
       "  61,\n",
       "  35,\n",
       "  37,\n",
       "  46,\n",
       "  27,\n",
       "  48,\n",
       "  11,\n",
       "  30,\n",
       "  50,\n",
       "  26,\n",
       "  39,\n",
       "  11,\n",
       "  35,\n",
       "  41,\n",
       "  63,\n",
       "  30,\n",
       "  73,\n",
       "  35,\n",
       "  22,\n",
       "  11,\n",
       "  64,\n",
       "  48,\n",
       "  36,\n",
       "  41,\n",
       "  35,\n",
       "  14,\n",
       "  74,\n",
       "  79,\n",
       "  35,\n",
       "  43,\n",
       "  67,\n",
       "  11,\n",
       "  76,\n",
       "  41,\n",
       "  58,\n",
       "  121,\n",
       "  23,\n",
       "  19,\n",
       "  21,\n",
       "  18,\n",
       "  48,\n",
       "  39,\n",
       "  33,\n",
       "  77,\n",
       "  52,\n",
       "  11,\n",
       "  47,\n",
       "  31,\n",
       "  43,\n",
       "  26,\n",
       "  25,\n",
       "  52,\n",
       "  50,\n",
       "  13,\n",
       "  189,\n",
       "  24,\n",
       "  51,\n",
       "  55,\n",
       "  59,\n",
       "  29,\n",
       "  26,\n",
       "  49,\n",
       "  47,\n",
       "  38,\n",
       "  27,\n",
       "  8,\n",
       "  25,\n",
       "  37,\n",
       "  35,\n",
       "  51,\n",
       "  25,\n",
       "  45,\n",
       "  31,\n",
       "  36,\n",
       "  144,\n",
       "  10,\n",
       "  30,\n",
       "  48,\n",
       "  51,\n",
       "  11,\n",
       "  107,\n",
       "  52,\n",
       "  38,\n",
       "  36,\n",
       "  74,\n",
       "  93,\n",
       "  58,\n",
       "  47,\n",
       "  59,\n",
       "  4,\n",
       "  172,\n",
       "  31,\n",
       "  52,\n",
       "  67,\n",
       "  81,\n",
       "  14,\n",
       "  12,\n",
       "  83,\n",
       "  39,\n",
       "  40,\n",
       "  63,\n",
       "  47,\n",
       "  45,\n",
       "  38,\n",
       "  15,\n",
       "  35,\n",
       "  49,\n",
       "  28,\n",
       "  68,\n",
       "  44,\n",
       "  38,\n",
       "  23,\n",
       "  11,\n",
       "  270,\n",
       "  42,\n",
       "  72,\n",
       "  64,\n",
       "  58,\n",
       "  73,\n",
       "  47,\n",
       "  12,\n",
       "  41,\n",
       "  63,\n",
       "  27,\n",
       "  10,\n",
       "  178,\n",
       "  23,\n",
       "  37,\n",
       "  54,\n",
       "  22,\n",
       "  21,\n",
       "  33,\n",
       "  29,\n",
       "  14,\n",
       "  86,\n",
       "  39,\n",
       "  17,\n",
       "  16,\n",
       "  19,\n",
       "  31,\n",
       "  23,\n",
       "  42,\n",
       "  32,\n",
       "  61,\n",
       "  62,\n",
       "  34,\n",
       "  63,\n",
       "  21,\n",
       "  11,\n",
       "  46,\n",
       "  29,\n",
       "  26,\n",
       "  41,\n",
       "  22,\n",
       "  21,\n",
       "  26,\n",
       "  11,\n",
       "  17,\n",
       "  57,\n",
       "  31,\n",
       "  41,\n",
       "  66,\n",
       "  38,\n",
       "  191,\n",
       "  33,\n",
       "  63,\n",
       "  82,\n",
       "  75,\n",
       "  58,\n",
       "  144,\n",
       "  31,\n",
       "  36,\n",
       "  30,\n",
       "  17,\n",
       "  64,\n",
       "  28,\n",
       "  34,\n",
       "  7,\n",
       "  52,\n",
       "  23,\n",
       "  38,\n",
       "  50,\n",
       "  23,\n",
       "  25,\n",
       "  25,\n",
       "  36,\n",
       "  40,\n",
       "  44,\n",
       "  56,\n",
       "  40,\n",
       "  68,\n",
       "  32,\n",
       "  73,\n",
       "  36,\n",
       "  44,\n",
       "  63,\n",
       "  48,\n",
       "  42,\n",
       "  26,\n",
       "  109,\n",
       "  31,\n",
       "  23,\n",
       "  11,\n",
       "  26,\n",
       "  107,\n",
       "  41,\n",
       "  7,\n",
       "  37,\n",
       "  41,\n",
       "  20,\n",
       "  15,\n",
       "  32,\n",
       "  29,\n",
       "  34,\n",
       "  17,\n",
       "  4,\n",
       "  23,\n",
       "  50,\n",
       "  59,\n",
       "  18,\n",
       "  66,\n",
       "  83,\n",
       "  79,\n",
       "  25,\n",
       "  56,\n",
       "  32,\n",
       "  19,\n",
       "  119,\n",
       "  37,\n",
       "  19,\n",
       "  67,\n",
       "  47,\n",
       "  11,\n",
       "  145,\n",
       "  63,\n",
       "  30,\n",
       "  55,\n",
       "  55,\n",
       "  11,\n",
       "  12,\n",
       "  130,\n",
       "  35,\n",
       "  171,\n",
       "  28,\n",
       "  91,\n",
       "  64,\n",
       "  31,\n",
       "  38,\n",
       "  49,\n",
       "  57,\n",
       "  40,\n",
       "  66,\n",
       "  24,\n",
       "  30,\n",
       "  56,\n",
       "  34,\n",
       "  21,\n",
       "  51,\n",
       "  62,\n",
       "  38,\n",
       "  14,\n",
       "  131,\n",
       "  95,\n",
       "  12,\n",
       "  20,\n",
       "  39,\n",
       "  55,\n",
       "  31,\n",
       "  7,\n",
       "  20,\n",
       "  33,\n",
       "  30,\n",
       "  23,\n",
       "  13,\n",
       "  86,\n",
       "  48,\n",
       "  76,\n",
       "  68,\n",
       "  53,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  71,\n",
       "  41,\n",
       "  9,\n",
       "  112,\n",
       "  32,\n",
       "  33,\n",
       "  44,\n",
       "  52,\n",
       "  33,\n",
       "  45,\n",
       "  44,\n",
       "  59,\n",
       "  32,\n",
       "  30,\n",
       "  94,\n",
       "  55,\n",
       "  19,\n",
       "  48,\n",
       "  28,\n",
       "  19,\n",
       "  33,\n",
       "  35,\n",
       "  10,\n",
       "  44,\n",
       "  11,\n",
       "  40,\n",
       "  26,\n",
       "  26,\n",
       "  18,\n",
       "  28,\n",
       "  31,\n",
       "  10,\n",
       "  29,\n",
       "  31,\n",
       "  24,\n",
       "  43,\n",
       "  7,\n",
       "  40,\n",
       "  25,\n",
       "  55,\n",
       "  43,\n",
       "  21,\n",
       "  29,\n",
       "  18,\n",
       "  21,\n",
       "  24,\n",
       "  56,\n",
       "  11,\n",
       "  15,\n",
       "  63,\n",
       "  92,\n",
       "  54,\n",
       "  28,\n",
       "  44,\n",
       "  66,\n",
       "  58,\n",
       "  9,\n",
       "  119,\n",
       "  51,\n",
       "  92,\n",
       "  72,\n",
       "  23,\n",
       "  29,\n",
       "  100,\n",
       "  11,\n",
       "  87,\n",
       "  62,\n",
       "  66,\n",
       "  28,\n",
       "  74,\n",
       "  75,\n",
       "  43,\n",
       "  11,\n",
       "  44,\n",
       "  65,\n",
       "  81,\n",
       "  16,\n",
       "  12,\n",
       "  40,\n",
       "  66,\n",
       "  46,\n",
       "  16,\n",
       "  47,\n",
       "  29,\n",
       "  64,\n",
       "  21,\n",
       "  41,\n",
       "  64,\n",
       "  39,\n",
       "  40,\n",
       "  30,\n",
       "  25,\n",
       "  40,\n",
       "  71,\n",
       "  10,\n",
       "  49,\n",
       "  35,\n",
       "  62,\n",
       "  4,\n",
       "  49,\n",
       "  43,\n",
       "  42,\n",
       "  94,\n",
       "  30,\n",
       "  35,\n",
       "  33,\n",
       "  14,\n",
       "  77,\n",
       "  49,\n",
       "  90,\n",
       "  68,\n",
       "  43,\n",
       "  45,\n",
       "  112,\n",
       "  10,\n",
       "  144,\n",
       "  20,\n",
       "  98,\n",
       "  50,\n",
       "  62,\n",
       "  133,\n",
       "  131,\n",
       "  25,\n",
       "  13,\n",
       "  70,\n",
       "  76,\n",
       "  ...])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figure out the max length, and all lengths. All lengths helps to figure out where \n",
    "#to set your max length for the delete sentences function. \n",
    "max_len_sent, list_of_all_lengths = max_encoded_length(list_o_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick plot to show approx dist of sentence lengths in our corpus\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist(list_of_all_lengths, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found item length: 181\n",
      "Found item length: 189\n",
      "Found item length: 172\n",
      "Found item length: 270\n",
      "Found item length: 178\n",
      "Found item length: 191\n",
      "Found item length: 171\n",
      "Found item length: 151\n",
      "Found item length: 235\n",
      "Found item length: 161\n",
      "Found item length: 509\n",
      "Found item length: 174\n",
      "Found item length: 207\n",
      "Found item length: 248\n",
      "Found item length: 180\n",
      "Found item length: 155\n",
      "Found item length: 165\n",
      "Found item length: 173\n"
     ]
    }
   ],
   "source": [
    "#Delete sentecnes that are too long.\n",
    "short_tokens, short_labels = delete_sentences(list_o_tokens, list_o_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Long term we should absolutely consider going back to the individual labels. Might need\n",
    "#to have more data than we currently do. Definitely eventually want to separate organic \n",
    "#and inorganic chemicals once we get a high-performing model. \n",
    "small_label_mapping = {'O': 3, \"B-MOL-O\": 1, \"I-MOL-O\": 1, \"E-MOL-O\": 1,\n",
    "                \"S-MOL-O\": 1, \"B-MOL-I\": 1, \"I-MOL-I\": 1, \"E-MOL-I\": 1,\n",
    "                \"S-MOL-I\": 1, \"B-PRO\": 2, \"I-PRO\": 2, \"E-PRO\": 2, \"S-PRO\": 2,\n",
    "                'PAD': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  In the interaction between gas molecules with single-walled carbon nanotube (SWCNT) we show that as a result of collisions the gas scattering contributes with an important background signal and should be considered in SWCNT-based gas sensors.\n",
      "Tokenized IDs:  tensor([  101,  1999,  1996,  8290,  2090,  3806, 10737,  2007,  2309,  1011,\n",
      "        17692,  6351, 28991, 28251,  2063,  1006, 25430,  2278,  3372,  1007,\n",
      "         2057,  2265,  2008,  2004,  1037,  2765,  1997, 28820,  1996,  3806,\n",
      "        17501, 16605,  2007,  2019,  2590,  4281,  4742,  1998,  2323,  2022,\n",
      "         2641,  1999, 25430,  2278,  3372,  1011,  2241,  3806, 13907,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "Extended Labels:  tensor([0, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "input_tokens, attention_mask, input_labels = tokenize_and_align_lables(short_tokens, short_labels, small_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we have our tokenized values, we'll build a dataloader to save on memory.\n",
    "#If you find your computer unable to handle this next bit, lower batch size to 16.\n",
    "#Likely you will see a performance hit, but it's better than it not working!\n",
    "dataset = TensorDataset(input_tokens, attention_mask, input_labels)\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset,\n",
    "                        sampler = RandomSampler(dataset),\n",
    "                        batch_size = batch_size\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,144 training samples\n",
      "  128 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_tokens, attention_mask, input_labels)\n",
    "\n",
    "#Do our 90/10 training/validation split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "#Now do a train_val split, randomly\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a dataloader batch size. Paper says do 16 or 32, but do bigger if comp can\n",
    "batch_size = 32\n",
    "\n",
    "#Create training dataloaders using a random sequence pull\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             sampler = RandomSampler(train_dataset),\n",
    "                             batch_size = batch_size\n",
    "                             )\n",
    "\n",
    "#Validation can go any order, so we'll do sequentially\n",
    "validation_dataloader = DataLoader(val_dataset,\n",
    "                                  sampler = SequentialSampler(val_dataset),\n",
    "                                  batch_size = batch_size\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "# BERT and Classifier Model Construction\n",
    "\n",
    "Here will build up a BERT model from the huggingface BERT base case, basically stacking a BERT model together with a NN classifier of some sort. - We'll start with a 1-layer just to get going. 3-layer seems a likely space for us to end up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        output_attentions = False, #Whether model returns attention weights\n",
    "        output_hidden_states = False, #Whether model outputs all hidden states                                    \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTplus1Layer(nn.Module):\n",
    "    #There's a more general version of this on the huggingface website.\n",
    "    #https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForTokenClassification\n",
    "    def __init__(self, p = 0.1):\n",
    "        super(BERTplus1Layer, self).__init__()\n",
    "        self.bert_segment = BertModel.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False\n",
    "        )\n",
    "        \n",
    "        self.classifier_dropout = nn.Dropout(p = p)\n",
    "        #We need a call to instantiate the weights here I think. \n",
    "        #Alternatively, we could have weights for our loss function I guess\n",
    "        self.classifier1 = nn.Linear(768, 4)\n",
    "    #This model setup actually loooks correct...? It instantiates pretty well.\n",
    "    #It's really basically what they have on the huggingface website. \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Think we'll need to handle having the tokens, labels, and att. mask.\n",
    "    #Not sure....\n",
    "    #You can pass multiple inputs, and return multiple outputs. \n",
    "    #Maybe I could have the loss calculated here too, so I could return the\n",
    "    #Loss and predictions at the same time. \n",
    "    def forward(self, input_ids = None, attention_mask = None, labels = None):\n",
    "        bert_outputs = self.bert_segment(input_ids, attention_mask = attention_mask)\n",
    "        #Once we have the outputs, do the thing we did last time, where we take the \n",
    "        #output and get the embedding, and pump the embedding into the classifier.\n",
    "        sequence_outputs = bert_outputs[0]\n",
    "        \n",
    "        sequence_output = self.classifier_dropout(sequence_outputs)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        #Here, include the weight of the function\n",
    "        #Give 0 weight to 0,\n",
    "        #give inverse class count to everything else.\n",
    "        weight_list = np.empty(4)\n",
    "        key_count = np.zeros(len(set(labels)))        \n",
    "        #Pretty sure this needs nested i and j. \n",
    "        #THIS WILL BE SLOW. WE SHOULD INSTEAD PASS\n",
    "        #OUR LOSS FUNCTION WEIGHTS IN EVERY TIME\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                key_count[labels[i][j]] += 1\n",
    "        weight_list = (sum(key_count)/len(set(labels))*key_count)\n",
    "        weight_list[0] = 0\n",
    "                \n",
    "        loss_function = nn.CrossEntropyLoss(weight = weight_list)\n",
    "            \n",
    "        #If we have labels, we can calculate losses.\n",
    "        #Focus on building a loss that doesn't consider padding tokens\n",
    "        #Can't drop the padding or outside tokens just yet. \n",
    "        if labels is not None:\n",
    "            #Nested for loops to go through all outputs\n",
    "            for i in range(len(logits)):\n",
    "                for j in range(len(logits[i])):\n",
    "                    if attention_mask[i][j] == 0:\n",
    "                        pass\n",
    "                    #Need losses for all non-pad tokens\n",
    "                    elif attention_mask[i][j] == 1:\n",
    "                        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTplus1Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTplus1Layer(\n",
       "  (bert_segment): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier1): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of BERTplus1Layer(\n",
       "  (bert_segment): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier1): Linear(in_features=768, out_features=4, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter if the classifier never learns about padding tokens. The only important thing is that the classifier never guesses that anything is a padding token. If we set a weight of 0 to the padding token classifier output, then we can turn that off permanently. And, since we'll always know what is and isn't a padding token via the attention mask, then we're golden. It doesn't matter what it predicts them as, just that it never predicts anything that isn't a padding token as being a pad. \n",
    "\n",
    "As such, we can get away with only including non-pads in the loss, and never letting the model learn about pads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([1,3,5,6,7,8,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, p = 0.1):\n",
    "        super(Net, self).__init__()\n",
    "        self.classifier_dropout = nn.Dropout(p = p)\n",
    "        self.classifier = nn.Linear(768,4)\n",
    "    def forward(self, x):\n",
    "        x = self.classifier_dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 1. Largely cause I'm not sure my computer will survive\n",
    "#This could also be a major source of error. I'm thinking I'll try 2 next and pray.\n",
    "#When I ran for 20 epochs, the model saturated around 5 epochs with 2e-5 lr though.\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
